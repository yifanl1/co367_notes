\documentclass[titlepage]{article}
\title{CO367, Course Notes}
\author{Transcribed by Louis Castricato}
\usepackage{amsmath,amssymb}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{outlines}
\usepackage{amsthm}
\delimitershortfall-1sp

\newcommand\abs[1]{\left|#1\right|}

\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem*{cor}{Corollary}

\theoremstyle{definition}
\newtheorem{defn}{Definition}[section]
\newtheorem{conj}{Conjecture}[section]
\newtheorem{exmp}{Example}[section]

\theoremstyle{remark}
\newtheorem*{rem}{Remark}
\newtheorem*{note}{Note}


\begin{document}
\maketitle
\section*{Introduction}

Mathematical Optimization or Mathematical Programming

Informally: Find a best solution to the model of a problem

*best* according to a given objective/criterion

Applications include
\begin{outline}[enumerate]
\1 Operations research
\2 Scheduling + Planning
\2 Supply Chain Management
\2 Vehicular Routing
\2 Power Grid Optimization
\1 Statistics and Machine Learning
\2 Curve FItting
\2 Classification, Clustering, SVM,...
\2 Deep Learning
\1 Finance
\1 Optimal Control
\1 Biology
\end{outline}
(OPT) min f(x) s.t.
\[g_i(x) \leq 0\text{, for } i \in\{1, 2, 3, \hdots, m\}\]
Remarks
\begin{itemize}
\item a. $\text{max} f(x) = -(\text{min} -f(x))$
\item b. $\{x \in \mathbb{R}^n:g(x) \geq 0\} = \{x \in \mathbb{R}^n : -g(x) \leq 0\}$
\item c. $\{x \in \mathbb{R}^n : g(x) \geq b\} = \{ x \in \mathbb{R}^n : -g(x) -  b \leq 0\}$
\end{itemize}
\subsection*{Classification of Problems - 1}
\begin{itemize}
\item if $f(x) = 0, \forall x  \in \mathbb{R}^n \implies$(OPT) is a feasibility
problem
\item if we have $m = 0$ constraints $\implies$ (OPT) is an unconstrained
optimization problem
\end{itemize}
\subsection*{Classification of Problems - 2}
Q: Why do we need f and g?
A: In abscence of hyp. on f and g, (OPT) is unsolvable.

\subsection*{Note: "Black box" optimization framework}
All that is given is an oracle function that can compute values of $f(x)$ $\forall
x$ in the domain of $f$\\\\
Example: consider
\begin{align*}
\text{min} f(x)\\
\text{s.t.} g(x) &\leq 0, \text{ for } i \in [1,m] \cap \mathbb{N}\\
h(x) &\leq 0\\
h(x), \text{ when } &x \in \mathbb{Z}^n\text{, do: } 0\\
h(x)\text{, do: } &1\\
\end{align*}
in other words, we only want integral solutions.
\begin{defn}
\textbf{Discrete Optimization: } When the constraint of OPT restrict to a lattice,
we have a discrete optimization problem
\end{defn}
\begin{defn}
\textbf{Continuous: } A function $f: D\mapsto \mathbb{R}$ is continuous over $D$
$(f \in C^k(D))$ if all its $k^{\text{th}}$ derivatives are continuous over $D$.
\end{defn}
Consider the following examples

\begin{align*}
f(x) \text{ when } x \geq 2 \text{, do: } 1\\
f(x)\text{, do: } -1
\end{align*}
Then $f(x)$ is not continuous.

In another example we have $g(x)$, do: abs$(x - 2)$. Then $g(x) \in C^0$.

\begin{defn}
\textbf{Gradient: } Let $f \in C^1(D)$ for $D \subseteq \mathbb{R}^n$. The
gradient is $\nabla f: D \mapsto \mathbb{R}^n$ if it satifies $\nabla f \in
C^0(D)$ and is given by $\nabla f(x) = \left[ \begin{array}{cc} \frac{\delta
f}{\delta x_1(x)} \hdots \ \frac{\delta f}{ \delta x_n(x)} \end{array} \right]$.
\end{defn}

\begin{defn}
\textbf{Hessian: } Let $f \in C^2(D)$ for $D\subseteq \mathbb{R}^n$. Its Hessian
is $\nabla^2 f: D\mapsto \mathbb{R}^n$. It satisfies $\nabla^2 f \in C^0(D)$ and
is given by
\[\nabla^2 f =\begin{bmatrix} \frac{\delta f(x)}{\delta x_1 \delta
x_1} & \hdots & \frac{\delta f(x)}{\delta x_n \delta x_1} \\ \vdots & \ddots &
\vdots \\ \frac{\delta f(x)}{\delta x_1 \delta x_n} & \hdots & \frac{\delta
f(x)}{\delta x_n \delta x_n}\end{bmatrix}\]
\end{defn}
\begin{defn}
\textbf{Linear: } A function $f: D \mapsto \mathbb{R}$, $D \subseteq \mathbb{R}^n$
is linear if $\exists c \in \mathbb{R}^n$ where $f(x) = c^T x, \forall x \in D$.
Then $\nabla f(x) = c$ and $\nabla^2 f(x) = 0$.
\end{defn}

\textbf{remark: } if $f, g_i$ are linear, then OPT is a linear programming
function.

\section{Linear Algebra}

A vector and matrix norm.

\begin{defn}
\textbf{Norm: } A norm $\|\cdot \|$ on $\mathbb{R}^n$ assigns a scalar $\|x\|$
to every $x \in \mathbb{R}^n$ $s.t.$
\begin{outline}[enumerate]
\1 $\|x\| \geq 0, \forall x \in \mathbb{R}^n$
\1 $\|cx\| = |c|\|x\|, \forall x \in \mathbb{R}^n \forall c \in \mathbb{R}$
\1 $\|x\| = 0 \iff x = 0$
\1 $\|x + y\| \leq \|x\| + \|y\|$ $\forall x,y \in \mathbb{R}^n$
\end{outline}
\end{defn}
\noindent $L^k$ norm $\|x\| = \big(\sum (x_i)^k\big)^{\frac{1}{k}}$ in particular,
\begin{outline}[enumerate]
\1 Manhattan Norm = $L_1$
\1 Euclidean Norm = $L_2$
\1 Infinite Norm = $L_{\infty} = \text{max}(|x_i|)$
\end{outline}
\noindent Schwartz inequaloty: $\forall x,y \in \mathbb{R}^n$

$|x^T y| \leq \|x\|_2 \cdot \|y\|_2$
\begin{thm}
\textbf{Pythagorean Theorem: } If $x,y \in \mathbb{R}$ are orthogonal then $\|x
+ y\|^2 = \|x\|^2 + \|y\|^2$ under $L_2$.
\end{thm}
\begin{defn}
\textbf{Matrix Norm: } Given a vector norm $\| \cdot \|$, the induced magtrix
norm associates a scalar $\|x\|$ to all $A \in \mathbb{R}^{n \times n}$

$\|A\| = \text{max} \|Ax\|$ where $\|x\| = 1$
\end{defn}

\noindent \textbf{Property of Matrix norm:}

$\|A\|_2 = \text{max}\|Ax\|_2 = \text{max}|y^TAx|$, where $\|x\|_2 = 1$ and $\|y\|_2
= 1$.

\indent \indent Proof is trivial by schwartz inequality.

$\|A\| = \|A^T\|_2$

\noindent
\textbf{TFAE: }\footnote{The following are equivalent}
\begin{outline}[enumerate]
\1 $A$ is nonsingular
\1 $A^T$ is nonsingular
\1 $\forall x \in \mathbb{R}^n$ if $x \not = 0$ then $Ax \not = 0$.
\1 $\forall b \in \mathbb{R}^n$, $\exists x \in \mathbb{R}^n$ $s.t.$ $Ax = b$
and $x$ is unique
\1 The columns of $A$ are linearly independent
\1 The rows of $A$ are linearly independent
\1 A unique inverse of $A$ exists
\1 If $B$ is a matrix $s.t.$ an inverse of $B$ exists, then $(AB)^{-1} = A^{-1}
B^{-1}$
\end{outline}
\begin{defn}
\textbf{Eigenvalue: } The characteristic polynomial $\Phi: \mathbb{R} \mapsto
\mathbb{R}$ of $A \in \mathbb{R}^{n\times n}$ is $\Phi(\lambda) = \text{det}(A -
\lambda I)$. It has $n$ complex roots, the eigenvalues of $A$. GIven an
eigenvalue $\lambda$ of $A$, $x \in \mathbb{R}^n$ is its corresponding
eigenvector of $A$ if $Ax = \lambda x$.
\end{defn}

\noindent \textbf{Properties : } Given $A \in \mathbb{R}^{n \times n}$
\begin{outline}[enumerate]
\1 $\lambda$ is an eigenvalue $\iff$ $\exists$ a corresponding eigenvector $x$.
\1 $A$ is simuglar $\iff$ it has a zero eigenvalue
\1 If $A$ is triangular, then its eigenvalues are its diagonal elements
\1 If $S \in \mathbb{R}^{n \times n}$ is nonsingular and $B = SAS^{-1}$ then $A$
and $B$ have the same eigenvalues.
\1 If the eigenvalues of $A$ are $\{\lambda_1,\hdots, \lambda_n\}$ then
\2 the eigenvalues of $A + c I$ are $c + \lambda_1, \hdots, c + \lambda_n$.
\2 the eigenvalues of $A^k$ are $\lambda_1^k,\hdots, \lambda_n^k$. This
also holds for $k = -1$.
\2 the eigenvalues of $A^T$ are the same as the eigenvalues of $A$.
\end{outline}

\begin{defn}
\textbf{Spectral Radius: } The spectral radius $\rho(A)$ of $A \in \mathbb{R}^{n
\times n}$ is the maximum magnitude of its eigenvalues.
\end{defn}
\noindent \textbf{Property: }

\begin{lem}
For any induced norm, $\| \cdot \|$, $\rho(A) \leq \|A^k\|^{\frac{1}{k}}$
$\forall k \in \mathbb{N}$
\end{lem}

\noindent \textbf{Proof: } By defn, $\|A^k\| = \text{max}\|A^k y\| =
\text{max}\frac{\|A^k y\|}{\|y\|}$, where $\|y\| = 1$.

Let $\lambda$ be an eigenvalue of $A$, and $x$ its eigenvector. Then
\[    \|A^k\| \geq \frac{\|A^kx\|}{\|x\|} = \frac{\|A^{k-1}Ax\|}{\|x\|} =
\frac{A^{k-1}\lambda x}{\|x\|} = \hdots = \frac{\|\lambda^kx\|}{\|x\|} =
\frac{(|\lambda^k|\|x\|)}{\|x\|} = \|\lambda^k\|\]

So for any eigenvalue, $\|A^k\| \geq | \lambda^k | \implies
\|A^k\|^{\frac{1}{k}} \geq \lambda \implies \rho(A) \leq \|A^k\|^{\frac{1}{k}}$.

\begin{lem}
For any induced norm, $\| \cdot \|$, $\lim_{k \to \infty} \|A^k\|^{\frac{1}{k}}
= \rho(A)$. Furthermore, $\lim_{k \to \infty} A^k = A$ iff $\rho(A) \leq 1$.
\end{lem}

\noindent \textbf{Proof: } Exercise!\\\\
\textbf{Symmetrix Matricies: }

\noindent \textbf{Property: } Let $A \in \mathbb{R}^{n\times n}$ be a symmetric matrix.
\begin{outline}[enumerate]
\1 its eigenvalues are real
\1 its eigenvectors are mutually orthongal
\1 assume its eigenvectors are normalized. Let $(\lambda_i, v_i)$ refer to an
eigenpair. Then $A = \sum \lambda_i x_i x_i^T$.
\end{outline}

\noindent \textbf{Proof: } Exercise!

\begin{lem}
Let $A \in \mathbb{R}^{n \times n}$ be a symmetric matrix, then $\|A\|_2 =
p(A)$.\\
\end{lem}
\noindent \textbf{Proof: } from before, $\rho(A) \leq \|A^k\|^{\frac{1}{k}}$ and
in particular we have that $p(A) \leq \|A\|_2$. Now all we need to do is show
that $p(A) \geq \|A\|_2$.

\noindent As the eigenvectors $x_i$ $i = 1, \hdots, n$ of C are mutually
orthogonal we can write any $y \in \mathbb{R}^n$ as $y = \sum \beta_i x_i$ for
some $\beta \in \mathbb{R}^n$.\\\\

\noindent By pythagoras' theorem, $\|y\|_2 = \sum \beta_i^2 \cdot \|x\|_2^2$.
Hence $Ay = A \sum \beta_i^2 \cdot \|x\|_2^2 = \sum \beta_i \lambda_i x_i$.
Again we can apply pythagoras'
\begin{align*}
\|Ay\|_2^2 &= \|\sum \beta_i \lambda_i x_i\|_2^2\\
&= \sum \beta_i \lambda_i^2 \|x\|_2^2\\
&= \sum |\lambda_i|^2 \cdot |\beta_i|^2 \cdot \|x\|_2^2\\
&\leq \sum\rho(A)^2 |\beta_i|^2 \|x\|_2^2\\
&= \rho(A)^2 \sum |\beta_i|^2 \|x\|_2^2\\
&= \rho(A)^2 \|y\|_2^2
\end{align*}
This then implies that
\begin{align*}
&\|A\|_2 \leq \rho(A) \|y\|_2\\
\implies &A  = \text{max} \frac{\|Ay\|_2}{\|y\|_2} \leq \frac{(\rho(A)
\|y\|_2)}{\|y\|_2} \text{, where $y \not = 0$}\\
\implies &\|A\|_2 \leq \rho(A)
\end{align*}
Therefore $\|A\|_2 = \rho(A)$.\\\\
\begin{lem}
Let $A \in \mathbb{R}^{n \times n}$ be symmetric, with eigen values $\lambda_1
\leq \hdots \leq \lambda_n \in \mathbb{R}$. Then $\forall y \in \mathbb{R}^n$ we
have that $\lambda_1 \|y\|_2^2 \leq y^T A y \leq \lambda_n \|y\|_2^2$.
\end{lem}

\noindent \textbf{Proof: } Express y as $\sum \beta_i x_i$, $i = 1,\hdots,n$
where $\beta_i \in \mathbb{R}$, $x_i$ are orthongal eigenvectors of $A$.
Firstly:

\[y^TAy = (\sum \beta_i x_i)^T (\sum \beta_i \lambda_i x_i) = \sum \beta_i^2
\lambda_i \|x_i\|_2^2\]
WLOG, assume that $\|x_i\|_2 = 1$ by normalization. So $y^T A y = \sum \lambda
\beta_i^2$. Secondly:

$\|y\|_2^2 = \sum \beta_i^2$\\

$\sum \lambda_1 \beta_1^2 \leq \sum \lambda_i \beta_i^2 \leq \sum \lambda_n
\beta_n^2 \implies \lambda_1 \|y\|_2^2 \leq y^T A y \leq \lambda_n \|y\|_2^2$.
\begin{lem}
Let $A \in \mathbb{R}^{n\times n}$ be symmetric, then $\|A^k\|_2 = \|A\|_2^k$.
\end{lem}

\noindent \textbf{Proof: }
Since $A$ is symmetric, we have that $(A^k)^T = A^k$ and $\|A^k\|_2 =
\rho(A^k)$. So $\rho(A^k) = \rho(A)^k$. Therefore $\|A\|_2^k =\|A^k\|_2$.
\begin{lem}
Let $A \in \mathbb{R}^{n\times n}$, then $\|A\|_2^2 = \|A^T A\|_2 = \|AA^T\|_2$.
\end{lem}

\noindent \textbf{Proof: } According to the schwartz inequality, $x^Ty \leq
\|x\|_2 \cdot \|y\|_2$
\begin{align*}
\|Ax\|_2^2 &= (Ax)^T(Ax) = (x^TA^T)(Ax) = x^T \cdot A^TAx\\
&\leq \|x\|_2  \cdot \|A\cdot  Ax\|_2\\
&\leq \|x\|_2 \cdot \|A^T A\|_2 \cdot \|x\|_2, \forall x \in \mathbb{R}^n
\end{align*}
\begin{rem}
\begin{align*}
\|A\|_2^2 &= \text{max}\frac{\|Ax\|_2^2}{\|X\|_2^2} \leq \|A^TA\|_2\\
\|A^TA\| &=\text{max} \|y^T A^T\|_2 \cdot \|Ax\|_2\\
&=(\text{max} \|y^TA^T\|_2)(\text{max}\|Ax\|_2)\\
&= \|A\|_2
\end{align*}
So we have that $\|A\|_2^2 = \|A^TA\|$. For $\|A\|_2^2= \|A A^T\|$ repeat steps
with $A$ amd $A^T$ swapped.
\end{rem}
\begin{lem}
For any $A \in \mathbb{R}^{m \times n}$, $A^TA$ is psd and $A^TA$ is pd iff
rank$(A) = n$
\end{lem}
\textbf{Proof: } The proof of this follows from the fact that a matrix with all positive
eigenvalues is pd, and a matrix with all positive/zero eigenvalues is psd.
Notice that $A^TA$ has all positive eigenvalues if rank$(A) = n$, and $A^TA$ has
all positive/zero eigenvalues otherwise. If required, showing that $A^TA$ has
all positive/zero eigenvalues can be done by multiplying their orthogonal
decompositions.
\begin{cor}
If $A$ is a square matrix, $A^TA$ is pd iff A is nonsingular.
\end{cor}
\textbf{Properties: }
\begin{outline}[enumerate]
\1 A square symmetric matrix is psd iff all of its eigenvalues are $\geq 0$
\1 A square symmetric matrix is pd iff all of its eigenvalues are $> 0$
\end{outline}
\textbf{Proof (For statement 1): } Let $\lambda$ be an eigenvalue of a psd
matrix $A$ and let $x$ be its corresponding nonzero eigenvector. Notice that
\begin{align*}
x^T A x &\geq 0, \text{ so } x^T \lambda x = \lambda \|x\|_2^2 \geq 0\\
\implies \lambda &\geq 0
\end{align*}

Let $\{\lambda_i\}$ refer to the set of eigenvalues of $A$ and let $\{x_i\}$
refer to its eigenvectors. As such, $\forall y \in \mathbb{R}^n$ $y$ is a
linear combination of $\{x_i\}$. Namely notice that we can write
\begin{align*}
y &= \sum \beta_i x_i\\
y^T A y &= (\sum \beta_i x_i)^T \sum \beta_i A x_i\\
&= (\sum \beta_i x_i)^T \sum \beta_i \lambda_i x_i\\
&= \sum \beta_i^2 \lambda \|x_i\|_2^2 \geq 0
\end{align*}
Statement 2 is left as an exercise to the reader.
\begin{cor}
The inverse of a pd matrix is also pd
\end{cor}
\noindent \textbf{Proof: } Trivial
\section{Convexity}
\begin{defn}
A set $C$ is called convex if it is closed under convex combinations. Namely
$\forall x,y \in C$, $\forall t \in [0,1]$ we have that $t x + (1 - t)y \in C$.
\end{defn}
\begin{defn}
A function $f$ is said to be convex if $f(\lambda x + (1 - \lambda)y) \leq
\lambda f(x) + (1 - \lambda)f(y)$ $\forall x,y \in D$, $\forall \lambda \in
[0,1]$.  A function $f$ is said to be strictly convex if $f(\lambda x + (1 -
\lambda)y) < \lambda f(x) + (1 - \lambda)f(y)$ $\forall x,y \in D$, $\forall \lambda \in
[0,1]$.
\end{defn}
\begin{defn}
A function $f$ is said to be concave if $f(\lambda x + (1 - \lambda)y) \geq
\lambda f(x) + (1 - \lambda)f(y)$ $\forall x,y \in D$, $\forall \lambda \in
[0,1]$. A function $f$ is said to be strictly convex if $f(\lambda x + (1 -
\lambda)y) > \lambda f(x) + (1 - \lambda)f(y)$ $\forall x,y \in D$, $\forall \lambda \in
[0,1]$.
\end{defn}
\begin{rem}
Notice that convex sets are closed under intersections. The Minkowski sum of
convex sets is convex. The image of a convex set under a linear transformation
is convex. The proof of all three properties is left as an exercise to the
reader.
\end{rem}
\begin{defn}
Let $f$ refer to a function with a convex domain $C$. The level sets of $f$ are
$\{x  \in C : f(x) \leq \alpha\}, \text{ } \forall \alpha \in R$.
\end{defn}
\begin{defn}
Same $f$ as above. The epigraph of $f$ is a subset of $\mathbb{R}^{n+1}$ given
by $\text{epi}(f) = \{(x,\alpha) : x \in C, \alpha \in R, f(x) \leq \alpha\}$.
\end{defn}
\begin{defn}
Same $f$ as above. The hypograph of $f$ is a subset of $\mathbb{R}^{n+1}$ given
by $\text{hypo}(f) = \{(x,\alpha) : x \in C, \alpha \in R, f(x) \geq \alpha\}$.
\end{defn}
\begin{rem} Some intuition. Notice that the intersection of the epi and hypo graph of a function is quite
literally the graph of said function. Furthermore the epigraph of a function
can be viewed as the region above the graph, inclusive, where as the hypograph
of a function can be viewed as the region below the graph, inclusive.
\end{rem}
\textbf{Properties}
\begin{outline}[enumerate]
\1 If $f: C \to R$ is convex, then its level sets are also convex. The converse
is not true
\2 Consider the example of $f(x) = \sqrt{|x|}$.
\1 $f: C \to R$ is convex iff its epigraph is a convex set.
\1 $f: C \to R$ is concave iff its hypograph is a concave set.
\1 $f: C \to R$ is linear iff its both concave and convex.
\1 The sum of two convex functions is also convex
\1 The sum of two concave functions is also concave
\1 The max of two convex functions is a convex (piecewise) function
\1 The max of two concave functions is a concave (piecewise) function
\1 Any vector norm is convex
\end{outline}
We'll prove the last statement and leave the rest as an exercise to the
reader.\\\\
\noindent \textbf{Proof: } This proof relies on the fact that norms satisfy the
triangle inequality. Let $f(x) = \|x\|$. Then notice that $\forall x,y \in
D$, $\forall \lambda \in [0,1]$ we have that
\begin{align*}
&f(\lambda x + (1 - \lambda) y)\\
&= \|\lambda x + (1 - \lambda) y\|\\
&\leq \lambda\|x\| + (1 - \lambda)\|y\|\\
&= \lambda f(x) + (1 - \lambda) f(y)
\end{align*}
\begin{thm}
Taylor's theorem for univariate functions.\\\\ Let $f:D\to R$.
\[f(x + h) = \sum\frac{h_i}{i!} f^i(x)+ \Phi(h)\]
where $\Phi$ refers to the residual function. Namely
\[\Phi(h) = \frac{h^{k + 1}}{(k + 1)!} f^{k + 1}(x + \lambda h)\]
for some $\lambda \in [0,1]$. Furthermore
\[\lim_{h \to 0} \frac{\Phi(h)}{h^k} = 0\]
\end{thm}
\begin{thm}
Taylor's theorem for multivariate functions.\\\\ Let $f:D^m \to R$.
\[f(x + h) = f(x) + h^T \nabla f(x) + \Phi(h)\]
where $\Phi$ refers to the residual function. Namely
\[\Phi(h) = \frac{1}{2} h^T \nabla^2 f(x + \lambda h) h\]
for some $\lambda \in [0,1]$. Furthermore
\[\lim_{h \to 0} \frac{\Phi(h)}{\|h\|} = 0\]
\end{thm}
\begin{thm}
2nd order
\[f(x + h) = f(x) + h^T \nabla f(x) + \frac{1}{2} h^T \nabla^2 f(x)h + \Phi(h)\]
\[\lim_{h \to 0} \frac{\Phi(h)}{\|h\|} = 0\]
\end{thm}
Notice that Taylor's theorem for univariate functions can be easily derived from
Taylor's theorem for multivariate functions and vice versa.
\begin{thm} Mean value Theorem\\\\
Let $f : D \to R$ be $C^1$ smooth. Then $\forall x,y \in D$ $\exists z \in
[x,y]$ suuch that $f(y) = f(x) + \nabla f(z)(y - x)$.
\end{thm}
\noindent \textbf{Proof: } follows from the zeroth order taylor expansion
\begin{defn}
The directional derivative of $f$ in direction $y$ is given by
\[\nabla_y f(x) = \lim_{\alpha \to 0} \frac{f(x + \alpha y) - f(x)}{\alpha}\]
\end{defn}
\begin{defn}
The gradient of $f$ is given by
\[\nabla f = (\nabla_{e_1} f(x), \hdots, \nabla_{e_k} f(x))\]
\end{defn}
\begin{cor}
If $f$ is $C^1$ smooth, the directional derivative of $f$ in direction $y$ can be computed as
\[\nabla_y f = y \cdot \nabla f\]
\end{cor}
\noindent \textbf{Proof: } Left as an exercise to the reader.
\begin{lem}
Let $C$ be convex. and let $f$ be differentiable over $C$. $f$ is convex iff
\[f \geq f(x) + (z - x)^T \nabla f(x), \text{ } \forall x,y \in C\]
\end{lem}
\begin{rem}
This is the most important theorem of this chapter!! Make sure you understand
it.
\end{rem}
\noindent \textbf{Proof: }

$(\implies)$ \\\\
As $C$ is convex, $x + (z - x)\alpha = \alpha z  + (1 - \alpha)x \in C,$ $\forall
\alpha \in [0,1]$
\[\lim_{\alpha \to 0} \frac{f(x + \alpha(z - x)) - f(x)}{\alpha} = (z - x)\nabla
f(x)\]
But by convexity
\[{f(x + \alpha(z - x)) - f(x)}{\alpha} \leq f(z) - f(x)\]
Taking the limit of $\alpha \to 0$ of both sides gets us the desired result.\\

$(\impliedby)$\\\\
Assume that $f(z) \geq f(x) + (z - x)^T \nabla f(x)$. Let $a,b \in C$ be any
points in the domain of $f$ and let $c = \alpha a + (1 - \alpha) b$. We can
write
\begin{outline}[enumerate]
\1 $f(a) \geq f(c) + (a - c)^T \nabla f(c)$
\1 $f(b) \geq f(c) + (b - c)^T \nabla f(c)$
\end{outline}
Multiply 1 by $\alpha$ and 2 by $(1 - \alpha)$ and sum them
\begin{align*}
\alpha f(a) + (1 - \alpha) f(b) &\geq \alpha (f(c) + (a - c)^T \nabla f(c)) + (1 -
\alpha) (f(c) + (b - c)^T \nabla f(c))\\
\alpha f(a) + (1 - \alpha) f(b) &\geq f(c) + \alpha (a - c)^T \nabla f(c) + (1 -
\alpha) (b - c)^T \nabla f(c)\\
\alpha f(a) + (1 - \alpha) f(b) &\geq f(c) + (\alpha a - \alpha c + b - \alpha b
- c + \alpha c)^T \nabla f(c)\\
\alpha f(a) + (1 - \alpha) f(b) &\geq f(c)\\
\alpha f(a) + (1 - \alpha) f(b) &\geq f(\alpha a + (1 - \alpha) b)
\end{align*}
Therefore, f is convex over $C$.\\
\begin{rem} Drawing out what this theorem is describing aids in forming an
intuition.
\end{rem}
\noindent \textbf{Properties: } Assume that $f$ is $C^2$ smooth.
\begin{outline}[enumerate]
\1 If $\nabla^2 f$ is psd, then $f$ is convex
\1 If $\nabla^2 f$ is pd, then $f$ is strictly convex.
\1 If the domain of $f$ is $\mathbb{R}^n$ and $f$ is convex over $D$, then
$\nabla^2 f(x)$ is psd $\forall x \in D$
\end{outline}
The proof of these properties is left as an exercise to the reader.
\end{document}
