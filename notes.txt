Non-linear optimization

Prof. Laurent Poirrier

Texts:
Nocedal/Wright: Numerical Optimization
Boyd/Vandenberghe: Convex Optimization

=LEC1= 2018-09-07
_CH0-Introduction_

 Mathematical Optimization or Mathematic Programming
  Informally: Find a best solution to the model of a problem
   *best* according to a given objective/criterion
 
 Applications
  Operations Research
   Scheduling + Planning
   Supply Chain Management
   Vehicular Routing
   Power Grid Optimization
  Statistics & Machine Learning
   Curve Fitting
   Classification, Clustering, SVM, ...
   Deep Learning
  Finance
  Optimal Control
  Biology
 
 
 (OPT) min f(x)
  s.t. g_i(x) ≤ 0, for i ∈ {1, 2, 3, ..., m}
  x ∈ R
 
 Remarks:
  a. max f(x) = -(min -f(x))
  b. {x ∈ Rⁿ: g(x) ≥ 0} = {x ∈ Rⁿ: -g(x) ≤ 0}
  c. {x ∈ Rⁿ: g(x) ≥ b} = {x ∈ Rⁿ: -g(x) - b ≤ 0}
 
 A. Classification of Solutions
  defn Open Ball:
   the open ball of radius δ around x_bar is B_δ(x_bar) = {x ∈ Rⁿ: ‖x - x_bar‖ ≤ δ}
 
  defn Minimizer:
   Consider f: D → R. The point x* ∈ D is
    a global minimizer for f on D if 
     f(x*) ≤ f(x), ∀x ∈ D
 
    a strict global minimizer for f on D if 
     f(x*) < f(x), ∀x ∈ D, x ≠ x*
    
    a local minimizer for f on D if
     ∃δ > 0: f(x*) ≤ f(x), ∀x ∈ B_δ(x*) ∩ D
 
    a strict local minimizer for f on D if
     ∃δ > 0: f(x*) < f(x), ∀x ∈ B_δ(x*) ∩ D, x ≠ x*
 
 B. Classification of Problems - 1
  a. if f(x) = 0 ∀x ∈ Rⁿ ⇒ (OPT) is a feasibility problem
  b. if we have m = 0 constractins ⇒ (OPT) is an unconstrained optimization problem
 
 C. Classification of Problems - 2
  Q: why do we need f and g?
  A: in the absence of hypotheses on f and g, (OPT) is unsolvable
 
  Note: "Black box" optimization framework
  All that is given is an oracle function that can compute values of f(x) for any x 
   (and possibly some extensions to compute derivatives)

 Example: solve 
  min f(x)
  s.t. x ∈ R
  f(x) := 0 when x = λ
  f(x) := 1 otherwise

=LEC2= 2018-09-10
 Example: consider
  min f(x)
  s.t. g(x) ≤ 0, for i ∈ [1, m]
       h(x) ≤ 0
  h(x) when x in Zⁿ, do: 0
  h(x), do: 1

  in other words, we only want integral solutions
 
 defn Discrete Optimization
  When the constraints of (OPT) restrict to a lattice, we have a discrete optimization problem


 defn Continuous
  A function f: D → R is continuous over D 
  if ∀ε > 0, ∃δ > 0 s.t. |x - y| < δ ⇒ |f(x) - f(y)| < ε, ∀x, y ∈ D

 defn Smooth
  A function f: D → R is Cᵏ smooth over D (f ∈ Cᵏ(D)) 
  if all its kth derivatives are continuous over D

 f(x) when x >= 2, do: 1
 f(x), do: -1
  f(x) is discontinous

 g(x), do: abs(x - 2)
  g(x) ∈ C⁰

 h(x) when x >= 2, do: 1/2 (x-2) ^ 2
 h(x), do: 1/2 (2-x) ^ 2
  h(x) ∈ C¹

 defn Gradient
  Let f ∈ C¹(D) for D ⊆ Rⁿ. The gradient is
   ∇f: D → Rⁿ
   if satisfies ∇f ∈ C⁰(D) and is given by ∇f(x) = [δf/δx_1(x), ..., δf/δx_n(x)]~

 defn Hessian
  Let f ∈ C²(D) for D ⊆ Rⁿ. Its Hessian is
   ∇²f: D → Rⁿ
   It satisfies ∇²f ∈ C⁰(D) and is given by ∇²f = 
     | δf(x)/δx_1δx_1 ... δf(x)/δx_nδx_1 |
     | ...            ...            ... |
     | δf(x)/δx_1δx_n ...  δf(x)/δx_nδx_n|

 defn Linear
  A function f: D → R, D ⊆ Rⁿ is linear 
  if ∃c ∈ Rⁿ where f(x) = c~x, ∀x ∈ D
  Then ∇f(x) = c and ∇²f(x) = [0] for all x ∈ D

 remark: if f, g_i are linear, then (OPT) is a linear programming function

_CH1-Linear Algebra_
 A vector and matrix norm

 defn Norm
  A norm ‖.‖ on Rⁿ assigns a scalar ‖x‖ to every x ∈ Rⁿ s.t.
   1) ‖x‖ ≥ 0, ∀x ∈ Rⁿ
   2) ‖cx‖ = |c|‖x‖, ∀c ∈ R, ∀x ∈ Rⁿ
   3) ‖x‖ = 0 ⟺ x = 0 
   4) ‖x + y‖ ≤ ‖x‖ + ‖y‖ ∀x, y ∈ Rⁿ

  Lᵏ norm ‖x‖ₖ = (∑(x_i)ᵏ)^1/k 
   in particular:
    Manhattan Norm: L₁ = ‖x‖₁ = ∑|x_i|
    Euclidean Norm: L₂ = ‖x‖₂ = √(∑x_i²)
    Infinite Norm: L_inf = ‖x‖_inf = max(|x_i|)

  Schwartz inequality: ∀x,y ∈ Rⁿ
   |x~y| ≤ ‖x‖₂ ⋅ ‖y‖₂
   equality when x = λy, for some λ ∈ R

  Pythagorean theorem: If x, y ∈ Rⁿ are orthogonal, then ‖x + y‖₂² = ‖x‖₂² + ‖y‖₂²

 defn Matrix Norm
  Given a vector norm ‖.‖, the induced matrix norm associates a scalar ‖A‖ to all A ∈ Rⁿˣⁿ
   ‖A‖ = max ‖A.x‖, when ‖x‖ = 1

=LEC3= 2018-09-12
  Property of Matrix norm:
   ‖A‖₂ = max ‖A.x‖₂ = max |y~Ax|
         ‖x‖₂ = 1     ‖x‖₂, ‖y‖₂ = 1 
   Proof: Schwartz inequality to |y~Ax|
  Property:
   ‖A‖₂ = ‖A~‖₂
   pf: swap x and y in above property

  Properties: Let A ∈ Rⁿˣⁿ, following are equaivalent
   a) A is non-singular
   b) A~ is non-singular
   c) ∀x ∈ Rⁿ, if x ≠ 0, Ax ≠ 0
   d) ∀b ∈ Rⁿ, ∃x ∈ Rⁿ s.t. Ax = b, and x is unique
   e) the columns of A are linearly independent
   f) the rows of A are linearly independent
   g) ∃B ∈ Rⁿˣⁿ such that AB = I = BA, where B is unique (B is the inverse of A)
   h) for all A, B ∈ Rⁿˣⁿ, (AB)` = B`A` if B` exists

 defn Eigenvalue
  The characteristic polynomial Φ: R → R of A ∈ Rⁿˣⁿ is Φ(λ) = det(A - λI)
  It has n complex roots, the eigenvalues of A
  Given an eigenvalue λ of A, x ∈ Rⁿ is its corresponding eigenvector of A if Ax = λx

  Properties: Given A ∈ Rⁿˣⁿ
  a) λ is an eigenvalue iff ∃ a corresponding eigenvector x
  b) A is singular iff it has a zero eigenvalue
  c) if A is triangular, then its eigenvalues are its diagonal elements
  d) if S ∈ Rⁿˣⁿ is non-singular, and B = SAS`, then A and B have the same eigenvalues
  e) if the eigenvalues of A are λ_1, ..., λ_n (not necessarily distinct)
     - the eigenvalues of A + cI are c+λ_1, ..., c + λ_n
     - the eigenvalues of Aᵏ are λ_1ᵏ, ..., λ_nᵏ
     - the eigenvalues of A` are 1/λ_1, ..., 1/λ_n
     - the eigenvalues of A~ are λ_1, ..., λ_n

 defn Spectral Radius
  the spectral radius ρ(A) of A ∈ Rⁿˣⁿ is the maximum of the magnitudes of its eigenvalues

  Property: 
   For any induced norm ‖.‖, ρ(A) ≤ ‖Aᵏ‖^1/k, for k = 1, 2, ...
   
   pf: 
    By definition, ‖Aᵏ‖ = max ‖Aᵏy‖ = max ‖Aᵏy‖/‖y‖
                         ‖y‖=1       y≠0
    In particular, let λ be any eigenvalue of A, and x its eigenvector
    ‖Aᵏ‖ ≥ ‖Aᵏx‖/‖x‖ = ‖Aᵏ⁻¹Ax‖/‖x‖ = ‖Aᵏ⁻¹λx‖/‖x‖ = ... = ‖λᵏx‖/‖x‖ = (|λᵏ|‖x‖)/‖x‖ = |λᵏ|
    So for any eigenvalue, ‖Aᵏ‖ ≥ |λᵏ| ⇒ ‖Aᵏ‖^1/k ≥ λ ⇒ ρ(A) ≤ ‖Aᵏ‖^1/k, QED

  Property: 
   For any induced norm ‖.‖, lim k→∞ ‖Aᵏ‖^1/k = ρ(A)
   Also, lim k→∞ Aᵏ = 0 iff ρ(A) < 1

   pf: exercise!

 Symmetric Matrices
  Properties: Let A ∈ Rⁿˣⁿ be a symmetric matrix
   a) its eigenvalues are real
   b) its eigenvectors are n mutually orthogonal real nonzero vectors
   c) if the eigenvectors x₁, ..., xₙ are normalized s.t. ‖x‖₂ = 1, with corresponding λ₁, ..., λₙ, then A = ∑λᵢxᵢxᵢ~

   pf: exercise!

=LEC4= 2018-09-14
  Property: 
   Let A ∈ Rⁿˣⁿ be a symmetric matrix, then ‖A‖₂ = ρ(A)

   pf: 
    from before, ρ(A) ≤ ‖Aᵏ‖^1/k, in particular, ρ(A) ≤ ‖A^1‖₂^1/1 = ‖A‖₂
    Now, need to prove ρ(A) ≥ ‖A‖₂
    As the eigenvectors xᵢ, i = 1...n of C are mutually orthogonal
    we can write any y ∈ Rⁿ as y = ∑ βᵢxᵢ, i=1...n, for some βᵢ ∈ R

    By Pythagoras' theorem, ‖y‖₂² = ∑ βᵢ² . ‖x‖₂²; i=1...n
    So, Ay = A ∑βᵢxᵢ = ∑βᵢAxᵢ = ∑βᵢλᵢxᵢ
    Again using Pythagoras', ‖Ay‖₂² = ‖∑βᵢλᵢxᵢ‖₂² 
     = ∑βᵢ²λᵢ²‖x‖₂² 
     = ∑|λᵢ|² . |βᵢ|² . ‖x‖₂²
     ≤ ∑ρ(A)²|βᵢ|²‖x‖₂²       # by definition of spectral radius, λᵢ ≤ ρ(A)
     = ρ(A)² ∑|βᵢ|²‖x‖₂²
     = ρ(A)² ‖y‖₂²

    ‖Ay‖₂ ≤ ρ(A) ‖y‖₂
    ⇒ ‖A‖₂ = max ‖Ay‖₂/‖y‖₂ ≤ (ρ(A)‖y‖₂)/‖y‖₂
             y≠0
    ⇒ ‖A‖₂ ≤ ρ(A) 
    QED

  Property:
   Let A ∈ Rⁿˣⁿ be symmetric, with eigenvalues λ₁ ≤ ... ≤ λₙ ∈ R
   Then ∀y ∈ Rⁿ: λ₁‖y‖₂² ≤ y~Ay ≤ λₙ‖y‖₂²

   pf:
    write y as ∑βᵢxᵢ, i=1...n, where βᵢ ∈ R, xᵢ are orthogonal eigenvectors of A
    firstly: 
    y~Ay = (∑βᵢxᵢ)~(∑βᵢλᵢxᵢ) = ∑βᵢ²λᵢ‖xᵢ‖₂²
    wlog, assume ‖xᵢ‖₂ = 1 by normalization, so y~Ay = ∑λᵢβᵢ²

    secondly:
    ‖y‖₂² = ∑βᵢ²‖xᵢ‖² = ∑βᵢ²

    ∑λ₁βᵢ² ≤ ∑λᵢβᵢ² ≤ ∑λₙβᵢ² for any i = 1...n  # notice the subscript of λ
    ⇒ ∑λ₁‖y‖₂² ≤ y~Ay ≤ ∑λₙ‖y‖₂² 
    QED

  Property:
   Let A ∈ Rⁿˣⁿ be symmetric, then ‖Aᵏ‖₂ = ‖A‖₂ᵏ for any natural number k

   pf:
    As A is symmetric, A = A~
    (Aᵏ)~ = (A ... A)ᵏ = A~ ... A~ = A ... A = Aᵏ
    As Aᵏ is symmetric, ‖Aᵏ‖₂ = ρ(Aᵏ)
    The eigenvalues of Aᵏ are λ₁ᵏ, ..., λₙᵏ when λ₁, ..., λₙ are the eigenvalues of A
    So, ρ(Aᵏ) = ρ(A)ᵏ
    as ρ(A) = ‖A‖₂ ⇒ ‖A‖₂ᵏ = ‖Aᵏ‖₂

  Property:
   Let A ∈ Rⁿˣⁿ, then ‖A‖₂² = ‖A~A‖₂ = ‖AA~‖₂

   pf: 
    According to Schwartz inequality: x~y ≤ ‖x‖₂ . ‖y‖₂
    ‖Ax‖₂² = (Ax)~(Ax) = (x~A~)(Ax) = x~ . A~Ax
    ≤ ‖x‖₂ . ‖A~Ax‖₂ 
    ≤ ‖x‖₂ . ‖A~A‖₂ . ‖x‖₂, ∀x ∈ Rⁿ

    Remark:
     ‖A‖₂² = max ‖Ax‖₂²/‖x‖₂² ≤ ‖A~A‖₂
            x∈Rⁿ

=LEC5= 2018-09-17
     ‖A~A‖ = max |y~A~Ax|    
            ‖y‖=1,‖x‖=1
          ≤ max ‖y~A~‖₂ . ‖Ax‖₂
            ‖y‖=1,‖x‖=1
          = (max ‖y~A~‖₂) (max ‖Ax‖₂)
            ‖y‖=1        ‖x‖=1
          = ‖A‖₂²
     So, we have ‖A‖₂² = ‖A~A‖
     For ‖A‖₂² = ‖AA~‖, repeat steps with A and A~ swapped

  Property:
   ‖A`‖₂ is 1/λ₁ where λ₁ is the smallest magnitude eigenvalue of A

   pf:
    Remark that ‖A`‖₂ = ρ(A`), and the eigenvalues of A` are the inverses of the eigenvalues of A
    inverse of smallest = largest

 defn Positive Definite Matrix
  A symmetric matrix A ∈ Rⁿˣⁿ is 
   positive definite if x~Ax > 0 for all x ∈ Rⁿ, x≠0
   positive semidefinite if x~Ax ≥ 0 for all x ∈ Rⁿ

  Property:
   For any A ∈ Rᵐˣⁿ (not necessaily square), A~A is psd, and A~A is pd iff rank(A) = n

   pf:
    A~A is square and symmetric: 
     trivial
    A~A is psd: 
     for any x ∈ Rⁿ, x~(A~A)x = (Ax)~(Ax) = ‖Ax‖₂² ≥ 0
    A~A is pd iff rank(A) = n:
     x~A~Ax > 0 ∀x ∈ Rⁿ, x ≠ 0
     ⟺ ‖Ax‖₂² > 0 ∀x ∈ Rⁿ, x ≠ 0
     ⟺ ‖Ax‖₂ > 0 ∀x ∈ Rⁿ, x ≠ 0
     ⟺ Ax ≠ 0 ∀x ∈ Rⁿ, x ≠ 0
     ⟺ rank(A) = n            by fundamental theory of linear algebra

  Corollary:
   If A ∈ Rⁿˣⁿ is square, A~A is pd iff A is nonsingular

  Properties:
   1. A square symmetric matrix is psd iff all its eigenvalues are ≥ 0 
   2. A square symmetric matrix is pd iff all its eigenvalues are > 0 

   pf (For statement 1):
    Let λ be an eigenvalue of psd matrix A, and let x be its corresponding nonzero eigenvector
    x~Ax ≥ 0, so x~λx = λ‖x‖₂² ≥ 0 
    ⇒ λ ≥ 0

    Let λ₁, ..., λₙ ≥ 0 be the eigenvalues of A, 
     and let x₁, ..., xₙ be the n (nonzero, real, mutually orthogonal) eigenvectors
    As such, ∀y ∈ Rⁿ, y is a linear combination of (x₁, ..., xₙ)
    y = ∑βᵢxᵢ, i=1..n
    y~Ay = (∑βᵢxᵢ)~A(∑βᵢxᵢ)
    = (∑βᵢxᵢ)~(∑βᵢAxᵢ)
    = (∑βᵢxᵢ)~(∑βᵢλᵢxᵢ)
    = ∑βᵢ²λᵢ‖xᵢ‖₂² ≥ 0
    
    For statement 2, do it yourself

  Property:
   The inverse of a pd matrix is also pd
   
   pf:
    Let λ₁, ..., λₙ > 0 be the eigenvalues of A, then clearly their inverses (the eigenvalues of A`) must also be positive

_CH2-Convexity_
 defn Convex
  A set C ⊆ Rⁿ is convex if 
   λx + (1 - λy) ∈ C, ∀x,y ∈ C, ∀λ ∈ [0, 1]

=LEC6= 2018-09-19
  A function f: D → R is convex if
   f(λx + (1 - λ)y) ≤ λf(x) + (1 - λ)f(y) ∀x,y ∈ D, ∀λ ∈ [0, 1]
  f is strictly convex if strict inequality holds
   f(λx + (1 - λ)y) < λf(x) + (1 - λ)f(y) ∀x,y ∈ D, ∀λ ∈ [0, 1]

  Properties:
   - for any collection {cᵢ : i ∈ I} of convex sets, their intersection ∩cᵢ is convex
   - the vector (Minkowski) sum {x + y : x ∈ C₁, y ∈ C₂} of convex sets C₁, C₂ is convex
   - the image of a convex set under a linear transformation is convex

 defn Level Set
  Let f: C → R be a function, with a convex domain C
  the level sets of f are {x ∈ C: f(x) ≤ α} ∀α ∈ R

 defn Epigraph
  f as before
  the epigraph of f is a subset of Rⁿ⁺¹ given by epi(f) = {(x, α) : x ∈ C, α ∈ R, f(x) ≤ α}

  Properties
   a. if f: C → R is convex, then its level sets are convex
    The converse of a is not true
     the level sets of f(x) = √|a| are {x: -α² ≤ x ≤ α²}
     however, f(x) is not convex: let x = 0, y = 1, λ = 0.5
     f(0.5) = √0.5, 0.5f(0) + 0.5f(1) = 0.5
     √0.5 > 0.5!
   b. f: C → R is convex iff its epigraph is a convex set
   c. any linear function is convex (not necessarily strictly)
   d. if f is a convex function, g(x) = λf(x) is convex ∀λ ≥ 0
   e. the sum of two convex functions is a convex function
   f. the max of two convex functions is a convex (piecewise) function
   g. Any vector norm is convex
    pf:
     Let f(x) = ‖x‖, then for any x,y ∈ Rⁿ, λ ∈ [0, 1]:
      f(λx + (1 - λ)y) 
      = ‖λx + (1 - λ)y‖ 
      ≤ ‖λx‖ + ‖(1 - λ)y‖      # by definition of a norm
      = λ‖x‖ + (1 - λ)‖y‖ 
      = λf(x) + (1 - λ)f(y)

=LEC7= 2018-09-21
 Taylor's theorem for univariate functions
  f(x + h) = ∑[hⁱ/i! dᵢ(f(x))] + ϕ(h) for i = 0..k, where dᵢ(f) is the ith derivative of f
  Φ(h) = hᵏ⁺¹/(k + 1)! dₖ₊₁(f(x + λh)), λ ∈ [0, 1] is the residual function
  In particular, lim h→0 Φ(h)/hᵏ = 0

 Taylor's theorem for multivariate functions
  1st order (k = 1)
   f(x + h) = f(x) + h~∇f(x) + Φ(h)
   Φ(h) = 1/2 h~∇²f(x + λh)h, λ ∈ [0, 1]
   with lim h→0 Φ(h)/‖h‖ = 0

  2nd order
   f(x + h) = f(x) + h~∇f(x) + 1/2h~∇²f(x)h + Φ(h)
   with lim h→0 Φ(h)/‖h‖² = 0

 Mean Value theorem
  Let f: D → R, D ⊆ R, f ∈ C¹(D)
  then ∀x,y ∈ D, ∃z ∈ [x, y] f(y) = f(x) + ∇f(z)(y-x),
   pf: by zeroth order Taylor expansion

 defn Directional Derivative
  The directional derivative of f in the direction of y is 
   ∇_y f(x) = lim α→0 (f(x + αy) - f(x))/α

  In particular, ∇_eᵢ f(x) = δf/δxᵢ(x) and ∇f = (∇_e₁f(x) ... ∇_eₙf(x))~
  "direction" -> draw out the function!

 Thm:
  If f ∈ C¹, then ∇_hf = h~∇f

  pf:
   ∇_hf 
   = lim α→0 (f(x + αh) - f(x))/α 
   = lim α→0 (f(x) + αh~∇f(x) + Φ(αh) - f(x)) / α
   = lim α→0 (αh~∇f(x) + Φ(αh)) / α
   = lim α→0 αh~∇f(x) / α + lim α→0 Φ(αh) / α
   = h~∇f(x) + lim α→0 Φ(αh) / α  # see definition of residual above
   = h~∇f(x)

 Property:
  Let C ⊆ Rⁿ be convex, and let f: C → R be differentiable over C.
   f is convex iff
    f(z) ≥ f(x) + (z-x)~∇f(x) ∀x,z ∈ C
   pf:
    (->) As C is convex, x + (z-x)α = αz + (1-α)x ∈ C, ∀α ∈ [0, 1]
     lim→0 (f(x+α(z-x)) - f(x))/α = ∇_z-x f(x) = (z-x)~∇f(x)

     By convexity of f, f(x+α(z-x)) ≤ αf(z) + (1-α)f(x), ∀α ∈ [0, 1]
     f(x+α(z-x)) - f(x) ≤ αf(z) - αf(x)
     (f(x+α(z-x)) - f(x))/α ≤ f(z) - f(x)
     taking lim α→0:
     (z-x)~∇f(x) ≤ f(z) - f(x)

=LEC8= 2018-09-24
    (<-) if f(z) ≥ f(x) + (z-x)~∇f(x), ∀x,z ∈ C
     Let a,b ∈ C be any points in the domain of f, let c = αa + (1 - α)b
     (1) f(a) ≥ f(c) + (a - c)~∇f(c)
     (2) f(b) ≥ f(c) + (b - c)~∇f(c)

     multiply (1) by α and (2) by (1 - α), then add them:
      αf(a) + (1 - α)f(b) ≥ α(f(c) + (a - c)~∇f(c)) + (1 - α)(f(c) + (b - c)~∇f(c))
      αf(a) + (1 - α)f(b) ≥ f(c) + α((a - c)~∇f(c)) + (1 - α)((b - c)~∇f(c))
      αf(a) + (1 - α)f(b) ≥ f(c) + (αa - αc + b - αb - c + αc) ~∇f(c))
      αf(a) + (1 - α)f(b) ≥ f(c) + (αa + b - αb - c) ~∇f(c)) # by definition, c = αa + (1 - α)b
      αf(a) + (1 - α)f(b) ≥ f(c)
      αf(a) + (1 - α)f(b) ≥ f(αa + (1 - α)b)
     Therefore, f is convex over C.
  
  Properties: Let f: Rⁿ → R, f ∈ C²(D)
   a. ∇²f(x) psd ∀x ∈ D ⇒ f convex over D
    pf:
     for all x, y ∈ D, by 1st order Taylor
     f(y) = f(x) + (y - x)~∇f(x) + 1/2 (y-x)~∇²f(x + α(y-x))(y-x) for some α ∈ [0, 1]
   b. ∇²f(x) pd ∀x ∈ D ⇒ f strictly convex over D
    pf:
     similar to a, with y ≠ x, strict inequality.
   c. D = Rⁿ and f convex over D ⇒ ∇²f(x) psd ∀x ∈ D
    pf:
     f is convex over Rⁿ. Assume by contradiction, ∃x,z ∈ Rⁿ where z~∇²f(x) < 0
     because ∇²f is continuous, can find a z small enough that 
      z~∇²f(x + αz)z < 0 ∀α ∈ [0, 1] (1)
     By Taylor:
      f(x + z) = f(x) + z~∇f(x) + 1/2z~∇²f(x + βz)z for some 0 ≤ β ≤ 1
               < f(x) + z~∇f(x)       #by (1)
     Which contradicts convexity

_CH3-Optimality Conditions_
 Thm: [First order necessary conditions for optimality]
  Let f: Rⁿ → R be C¹-smooth
  If x* is a local minimizer, then ∇f(x*) = 0

  pf:
   let B_δ(x*) be such that f(x*) ≤ f(x) ∀x ∈ B_δ(x*)
   ∀i, ∀-δ < h < δ, f(x* + h.eᵢ) - f(x*) ≥ 0
   So, (f(x* + h.eᵢ) - f(x*))/h ≥ 0 if h > 0
       "                     " ≤ 0 if h < 0
   Because f ∈ C¹ lim h → 0 (f(x* + h.eᵢ) - f(x*))/h exists
   If its both >= 0 and <= 0, so = 0
   δf/δxᵢ(x*) = 0 ∀i, so ∇f(x*) = 0

 defn Critical/Stationary
  All x such that ∇f(x) = 0 are called critical or stationary points
  All local minimizers are critical points, but the converse is not true.

=LEC9= 2018-09-26
 Note: δ²f/δxᵢδxⱼ = δ²f/δxⱼδxᵢ, so ∇²f is symmetric

 Thm: [Second Order necessary conditions for optimality]
  Let f: Rⁿ → R be C²-smooth
  If x* is a local minimizer, then ∇f(x*) = 0, ∇²f(x*) is psd

  pf:
   Let z ∈ Rⁿ\{0}, need to prove z~∇²f(x*)z ≥ 0
   Let B_δ(x*) be f(x*) ≤ f(x), ∀x ∈ B_δ(x*)        # the ball
   Let y = hz/‖z‖ with 0 < h < δ

   f(x* + y) - f(x*) ≥ 0
   f(x*) + y~∇f(x*) + 1/2 y~∇²f(x*)y + ϕ(y) - f(x*) ≥ 0 where lim y → 0 ϕ(y)/‖y‖² = 0, y ≠ 0
   by 1st order condition: y~∇f(x*) = 0
   1/2 y~∇²f(x*)y + ϕ(y) ≥ 0
   1/2 h²/‖z‖² z~∇²f(x*)z + ϕ(hz/‖z‖) ≥ 0
   z~∇²f(x*)z + 2‖z‖²/h² ϕ(hz/‖z‖) ≥ 0
   z~∇²f(x*)z + 2‖z‖² lim h → 0 ϕ(hz/‖z‖)/h² ≥ 0
   z~∇²f(x*)z ≥ 0 ∀z QED
 
 Thm: [Second Order sufficient conditions for local optimality]
  Let f: Rⁿ → R, with f ∈ C²(B_δ(x*)). x* ∈ Rⁿ, δ > 0
  If ∇f(x*) = 0 and ∇²f(x*) is pd, then x* is a strict local minimizer

  pf:
   By Taylor 2-order: ∀h ∈ B_δ(0), f(x* + h) = f(x*) + h~∇f(x*) + 1/2h~∇²f(x*)h + ϕ(h)
   with lim h→0 ϕ(h)/‖h‖² = 0, h ≠ 0

   Let 0 < λ₁ < ... < λₙ be the eigenvalues of ∇²f(x*)
   ∃r > 0: ∀h∈B_r(0), |ϕ(h)/‖h‖²| ≤ λ₁/4
   ⇒ |ϕ(h)| ≤ ‖h‖²λ₁/4

   Notice that ‖y‖²λ₁ ≤ y~∇²f(x*)y ≤ ‖y‖²λ₂
   f(x*+h) = f(x*) + 1/2h~∇²f(x*)h + ϕ(h)
           ≥ f(x*) + 1/2‖h‖²λ₁ - 1/4‖h‖²λ₁ 
           = f(x*) + 1/4‖h‖²λ₁             # ‖h‖² > 0, λ₁ > 0
           > f(x*) ∀ h ∈ B_r(0)
   so x* is a strict local minimizer over B_r(x*), as required. QED

 Summary of necessary + sufficient optimality conditions
  a. ∇f(x*) = 0, ∇²f(x*) pd ⇒ x* is a strict local minimizer
  b. x* is a strict local minimizer ⇒ x* is a local minimizer
  c. x* is a local minimizer ⇒ ∇f(x*) = 0, ∇²f(x*) psd
  
  The converses of each statement are untrue!
   a. f(x) = x⁴, x* = 0, hessian not pd
   b. f(x) = 1, x* = 0, obviously not strict
   c. f(x) = x³, x* = 0, obviously not minimizer

=LEC10= 2018-09-28
 Thm: Let C ⊆ Rⁿ be a convex set, and f: C → R be a convex function
  A local minimixer of f is also a global minimizer
  If f is strictly convex, then there is at most one global minimizer
 
  pf:
   Suppose x* is a local minizizer, and y* is a global minimizer, with f(y*) < f(x*)
   By convexity of f:
    f(αy* + (1-α)x*) 
    ≤ αf(y*) + (1-α)f(x*) 
    = f(x*) - α(f(x*) - f(y*))
    < f(x*) ∀α ∈ [0, 1]
   Thus, ∀r > 0, ∃z ≠ x* s.t. ‖z - x*‖ < r and f(z) < f(x*)
   For instance, z = αy* + (1-α)x* with α = r/2 ‖y* - x*‖
   Thus, x* is not a local minimizer, contradiction
   So, f(y*) ≥ f(x*)

 Thm:
  ∀A ∈ Rⁿˣⁿ symmetric, ∃D, Q ∈ Rⁿˣⁿ such that
   a. D is diagonal, its diagonal entries are eigenvalues of A
   b. Q is orthogonal (Q` = Q~)
   c. A = QDQ

  pf:
   Let λ₁, ..., λₙ be the eigenvalues of A, and x₁, ..., xₙ be their corresponding eigenvectors
   Then 
    Axᵢ = λᵢxᵢ, i = 1..n
    A[x₁ x₂ ... xₙ] = [λ₁x₁ λ₂x₂ ... λₙxₙ] = [x₁ ... xₙ] diag(λ₁, ..., λₙ)
   As A is symmetric, the xᵢ vectors are mutually orthogonal, xᵢ.xⱼ when i ≠ j
   Let Q = [x₁ ... xₙ], then Q~Q = I, assuming wlog ‖xᵢ‖ = 1
   Let D = diag(λ₁, ..., λₙ), we get AQ = QD ⇒ A = QDQ` = QDQ~
 
 Thm:
  Let A ∈ Rⁿˣⁿ be symmetric
  A is psd iff it can be factored as A = GG~ for some G = Rⁿˣᵖ

  pf:
   (⇒) Assuming A = QDQ~ (by previous thm), where Q~ = Q` and D is diagonal
   Denote √D = diag(√(D₁₁), ... ,√(Dₙₙ)) and let G = Q√D
   THen GG~ = Q√D(Q√D)~ = Q√D√D~Q~ = QDQ~ = A 

   (⇐) Assume A = GG~
   Knowing that ∀M ∈ Rᵐˣⁿ, M~M is psd". Let M = G~.

 Observations:
  a. If √D = |d 0|, then G = Q√D = |Q₁₁ Q₁₂| |d 0| = |Q₁₁d 0|
             |0 0|                 |Q₂₁ Q₂₂| |0 0|   |Q₂₁d 0|
     So g = [Q₁₁d Q₂₁d]~ also satisfies gg~ = A

  b. If A is pd, then √D is invertible. Since Q is always invertible, we get that G = Q√D ∈ Rⁿˣⁿ is also invertible.

=LEC11= 2018-10-01
 defn Bounded
  A set S ⊆ Rⁿ is bounded if S ⊆ B_δ(0)
 
 defn Closed
  A set S is closde if for any sequence x₁, x₂, ... ∈ S such that lim i → ∞ xᵢ exists then lim i → ∞ xᵢ ∈ S
  
 defn Compact
  A set is compact if it is bounded and closed
 
 Thm [existence of a global minimizer]
  If S ⊂ Rⁿ is nonempty and compact and f: S → R is continuous, then ∃y, z ∈ S where f(y) ≤ f(x) ≤ f(z), ∀x ∈ S

 Thm
  If f is continuous, then its level sets are closed
  pf:
   Let S = {x ∈ Rⁿ: f(x) ≤ α} be any level set, for any sequence x₁, ... ∈ S, we have f(xᵢ) ≤ α. 
   By continuity of f, f(lim i→∞ xᵢ) = lim i→∞ f(xᵢ) ≤ α, thus lim i→∞ xᵢ ∈ S

 Thm
  If f: Rⁿ → R is continuous and has at least one bounded, nonempty level set, then f has a global minizer
  pf:
   Let S be the corresponding level set defined as {x ∈ Rⁿ: f(x) ≤ α}, and is given as bounded and nonempty
   By previous theorem, S is closed, so it is compact
   So, f must have a global minimizer over S: ∃y ∈ S: f(y) ≤ f(x)
   Consider all points x ∈ Rⁿ\S, we have f(x) > α ≥ f(y)
   Therefore, f(y) ≤ f(x) ∀x ∈ Rⁿ

 Functions w/o global minimizers:
  f(x) = 2x       #all level sets are unbounded
  f(x) = e^x      #all level sets are empty or unbounded
  
 defn Coercive
  A function f: Rⁿ → R is coercive if all its level sets are bounded

 note: unless f(x) = ∞ ∀x, if f is coercive, then it has a global minimizer

 Thm
  Let f: Rⁿ → R be a continuous function. f is coercive iff ∀r, ∃m > 0 such that ‖x‖ ≥ m ⇒ f(x) ≥ r
  pf:
   (⇐) consider S = {x ∈ Rⁿ: f(x) ≤ α}, by assumption, letting r = α + 1, we get ∃m > 0: ‖x‖ ≥ m ⇒ f(x) ≥ α + 1
   So, S ⊂ B_m(0), i.e. S is bounded.

   (⇒) For any given r, consider T = {x ∈ Rⁿ: f(x) ≤ r}
   By assumption, T is bounded, so ∃δ: T ⊆ B_delta(0)
   For all x: ‖x‖ ≥ δ + 1, x ∉ T, so f(x) > r
   Letting m = δ + 1, exactly the statement we want.

 Example:
  Let A ∈ Rᵐˣⁿ be of rank n. Show that f(x) = ‖Ax - b‖, with b ∈ Rᵐ is coercive.
  f(x) = ‖Ax - b‖ ≥ ‖Ax‖ - ‖b‖, by triangle inequality of norms

  f(x) ≥ ‖Ax‖ - ‖b‖ = √((Ax)~(Ax)) - ‖b‖ = √(x~A~Ax) - ‖b‖   #A~A is pd because A is full columnspace
  ≥ √(λ₁‖x‖²) - ‖b‖    # λ₁ is the smallest eigenvalue of A~A
  = √λ₁ ‖x‖ - ‖b‖
  ‖x‖ ≤ (f(x) + ‖b‖)/√λ₁
  So given r > 0, we have an f(x) > r, ∀‖x‖ ≥ r + ‖b‖/√λ₁

=LEC12= 2018-10-03
_CH4-Quadratic Functions_
 A quadratic function takes the form q(x) = x~Ax + b~x + c, for A ∈ Rⁿˣⁿ, b ∈ Rⁿ, c ∈ R 
 wlog, assume A is symmetric
 
 Thm 
  Let A ∈ Rⁿˣⁿ and let G be symmetric part of A, i.e. G = (A + A~)/2 
  a. G is symmetric
  pf
   G~ = [(A + A~)/2]~ = 1/2(A~ + A) = (A + A~)/2 = G
  b. q(x) = x~Gx + bx + c ∀x∈Rⁿ
   Observe that x~Ax is scalar, so (x~Ax)~ = x~Ax. 
   Then, x~Ax = 1/2(x~Ax) + 1/2(x~A~x)
   = 1/2(x~(A + A~)x) = x~Gx

 defn Range
  The range or column space of A ∈ Rᵐˣⁿ is Range(A) = {Ax: x ∈ Rⁿ}
 defn Null Space
  The null space or kernel of A ∈ Rᵐˣⁿ is Null(A) = {x ∈ Rⁿ: Ax = 0}

 Thm
  Let C ∈ Rᵐˣⁿ, if y ∈ Range(C~) and z ∈ Null(C), then y~z = 0
  pf: Since y ∈ Range(C~), ∃x ∈ Rⁿ: y = C~x. Then y~z = (C~x)~z = x~Cz = x~0 = 0

 Thm
  Let C ∈ Rᵐˣⁿ. ∀w ∈ Rⁿ, ∃y ∈ Range(C~) and z ∈ Null(C) unique such that w = y + z
  pf
   Let w = y + z + b, y ∈ Range(C~), z ∈ Null(C), be ∈ Range(C~)⟂ ∩ Null(C)⟂
   The decomposition is unique since Range(C~) ⟂ Null(C) ⟂ b
   Consider Cb ∈ Rᵐ. C~(Cb) ∈ Range(C~), so b ⟂ C~(Cb)
   Thus, 0 = b~(C~(Cb)) = (Cb)~(Cb) = ‖Cb‖₂²
   So Cb = 0, then b ∈ Null(C). We get b ∈ Null(C) ∩ Null(C)⟂, thus b must be 0.

 derivatives of q(x)
  a. δ/δxₖ b~x = bₖ, ∇b~x = b
  b. δ/δxₖ x~Ax 
   = δ/δxₖ ∑Aᵢⱼxᵢxⱼ 
   = δ/δxₖ (∑Aₖⱼxₖxⱼ + ∑Aᵢₖxᵢxₖ + Aₖₖxₖ²), where j ≠ k, i ≠ k
   = δ/δxₖ (2∑Aₖⱼxₖxⱼ + Aₖₖxₖ²)
   = 2∑Aₖⱼxⱼ + 2Aₖₖxₖ
   = 2∑Aₖⱼxⱼ
   = kth row of 2Ax
   ∇x~Ax = 2Ax
  c. ∇²bx = 0
  d. δ²/δxₖδxₗ x~Ax = δ/δxₗ(2∑Aₖⱼxⱼ) = 2Aₖₗ
   ∇²x~Ax = 2A

  Thm
   Given A ∈ Rⁿˣⁿ symmetric, b ∈ Rⁿ, c ∈ R, let q(x) = x~Ax + bx + c
   a. if A is pd, the q(x) has a unique global minimum x* = 1/2A`b
   b. if A is psd and b ∈ Range(A), then q(x) has a global minimizer
   c. otherwise, q(x) has no global minimizer

=LEC13= 2018-10-05
 Thm: 
  Given A ∈ Rᵐˣⁿ symmetric, be Rⁿ, c ∈ R, let q(x) = x~Ax + b~x + c
  a. if A is pd, q(x) has a unique global minimizer x* = -1/2 A`b
  b. if A is psd and b ∈ Range(A), q(x) has a global minimizer x*.
  c. otherwise, q(x) → -∞ for some ‖x‖ → ∞

  pf:
   Necessary conditions: x* local minimizer
   so ∇q(x*) = 0, ∇²q(x*) psd
   so 2Ax* + b = 0, A is psd

   a. Assume A is pd, thus, A` exists. There is a unique critical point 2Ax* + b = 0 ⇒ x* = -1/2A`b
    So, x* is a local minimizer because ∇²q(x*) = A is pd (and thus psd)

    q(x* + h) = (x* + h)~A(x* + h) + b~(x* + h) + c
    = x*~Ax* + x*~Ah + h~Ax* + h~Ah + b~x* + b~h + c # note that x*~Ah = (x*~Ah)~ = h~A~x* = h~Ax* when A is symmetric
    = q(x*) + 2h~Ax* + h~Ah + b~h
    = q(x*) + 2h~A(-1/2A`b) + h~Ah + b~h
    = q(x*) - h~b + h~Ah + b~h
    = q(x*) + h~Ah
    as A is psd, h~Ah ≥ 0
    so q(x*) ≤ q(x* + h) ∀h ∈ Rⁿ

   b. b ∈ Range(A) ⇒ -1/2b ∈ Range(A), so Ax* = -1/2b for some x*
    So x* satisfies 2Ax* + b = 0, and as given, A is psd.
    proof as before
    q(x* + h) = q(x*) + 2h~Ax* + h~Ah + b~h
    = q(x*) + 2h~(-1/2b) + h~Ah + b~h
    = q(x*) + h~Ah
    A is psd, h~Ah ≥ 0 ∀h ∈ Rⁿ 
    so x* is a global minimizer for q.
  c. Assume A is psd, but b ∉ Range(A)
   let b = y + z, uniquely with y ∈ Range(A~) = Range(A), z ∈ Null(A), z ≠ 0, since b ∉ Range(A)
   For any λ ∈ R, q(λz) = λ²z~Az + λb~z + c = 0 + λ(y + z)~z + c
    = λy~z + λz~z + c = λ‖z‖₂² + c
   for λ → -∞, q(λz) → -∞

   Assume A is not psd, then ∃v ∈ Rⁿ: v~Av < 0
   Let w ∈ Rⁿ with w = v if b~v ≥ 0, w = -v if b~v < 0. We have w~Aw < 0 and b~w ≥ 0.
   For any λ ∈ R, q(λw) = λ²w~Aw + λb~w + 0
   Taking λ → -∞, q(λz) → -∞

_CH5-Least Squares_
 Given a₁, ..., aₘ ∈ Rᵏ, b₁, ..., bₘ ∈ R, Find a function h: Rᵏ → R: h(aᵢ) ≈ bᵢ ∀i
 
 defn Least Square:
  minimize ∑(h(aᵢ) - bᵢ)²
  Determine the best h among a family of functions, parametrized by x∈R
  min ∑(hₓ(aᵢ) - bᵢ)²
   x ∈ Rⁿ

 Let f(x) = ∑(hₓ(aᵢ) - bᵢ)², min f(x)

 =LEC14= 2018-10-15
  a.
   Linear least squares
    hₓ(aᵢ) = x₁aᵢ₁ + ... + xₖaᵢₖ
   in 1 dimension, draw line hₓ(a) = ax through origin
  
   to get a line/hyperplane that does not contain the origin
    Let n = k + 1, a_i,k+1 = 1 ∀i
    then hₓ(aᵢ) = x₁aᵢ₁ + ... + xₖaᵢₖ + xₖ₊₁

    f(x) = ∑aᵢ~x + bᵢ)² = (Ax - b)~(Ax - b) = ‖Ax - b‖₂²
    = x~A~Ax - x~A~b - b~Ax + b~b
    = x~(A~A)x - (2A~b)~x + b~b, thus f(x) is a quadratic function

   If rank(A) = n, we see that ‖Ax - b‖₂ is coercive, so it has a global minimizer. 
   Also if rank(A) = n, A~A is pd, f(x) has a global minimizer x* = -1/2(A~A)`(-2A~b) = (A~A)`A~b

  b.
   Nonlinear least squares

   Let g: Rⁿ → Rᵐ, with gᵢ(x) = hₓ(aᵢ) - b
   We have f(x) = ∑(g(x))² = g(x)~g(x)

   defn Jacobian Matrix
    The Jacobian Matrix of g is given by 
     J(x) = [∇g₁(x)~ ... ∇gₘ(x)~]~

   δ/δxₖ f(x) = δ/δxₖ∑(gᵢ(x))²
    = ∑2 gᵢ(x) δ/δxₖgᵢ(x) = 2eₖ~J(x)~gᵢ(x). Thus, ∇f(x) = 2J(x)~g(x)
   
   Note that if g(x*) = 0, then ∇f(x*) = 0, and x* is a global minimizer
   δ²/δxₖδxⱼg(x) = δ/δxⱼ 2(∑gᵢ(x) δ/δxₖgᵢ(x)) = 2∑(δ/δxⱼgᵢ(x) δ/δxₖgᵢ(x) + gᵢ(x)δ²/δxₖδxⱼgᵢ(x))
   ∇²f(x) = 2∑gᵢ(x)∇²gᵢ(x) + 2J(x)~J(x)

_CH6-Descent Algorithms_
 General framework
  Choose x⁰ ∈ Rⁿ
  for k = 0, 1, ...
   Choose a search direction pᵏ ∈ Rⁿ
   Choose a step length αᵏ > 0

   Note: Objective f(xᵏ⁻¹) should be much smaller than f(xᵏ), and xᵏ converges very fast

 Steepest descent pᵏ = -∇f(xᵏ)

 Thm:
  Let f ∈ C¹(Bₜ(xᵏ)), t > 0 and ∇f(xᵏ) ≠ 0
  Consider the optimization problem min{f(xᵏ + εp)}, for some 0 < ε < t
  Let p* be a minimizer, then lim ε → 0 p*ε = -∇f(xᵏ)/‖∇f(xᵏ)‖

=LEC15= 2018-10-17

 Lemma: Let lim ε → 0, ε > 0 ϕ(εh)/ε = 0
  For any K > 0, there exists ε small enough s.t. |ϕ(εh)| ≤ εK

  pf
   For any K > 0, there exists γ > 0 s.t. |ϕ(εh)/ε| ≤ K, ∀0 < ε < γ
   Therefore |ϕ(εh)| ≤ εK ∀ε < γ
   Therefore, ε being sufficiently small means ε < γ

 Thm
  If ∇f(x*) ≠ 0, then for ε→0, min ‖p‖ = 1 {f(xᵏ + εp)} has optimum p* = -∇f(xᵏ)/‖∇f(xᵏ)‖₂

  pf
   Let x = xᵏ, p = -∇f(x)/‖∇f(x)‖₂, hence ∇f(x) = -p‖∇f(x)‖
   let u ∈ Rⁿ: ‖u‖₂ = 1, u ≠ p, so ‖u - p‖ > δ > 0

   Notice (u - p)~(u - p) > δ²
   So u~u - u~p - p~u + p~p > δ²
   2 - 2u~p > δ²
   1 - u~p > δ²/2
   u~p < 1 - δ²/2

   By Taylor: 
   f(x + εu) 
   = f(x) + εu~∇f(x) + ϕ(εu), with lim ε→0 ϕ(εu)/ε = 0
   = f(x) - ε‖∇f(x)‖u~p + ϕ(εu)
   ≥ f(x) - ε‖∇f(x)‖(1 - δ²/2) + ϕ(εu)
   = f(x) - ε‖∇f(x)‖ + εδ²/2‖∇f(x)‖ + ϕ(εu)

   By Lemma, |ϕ(εu)| ≤ ε‖∇f(x)‖δ²/4     # can use any arbitrary constant
   ϕ(εu) ≥ -ε‖∇f(x)‖δ²/4

   f(x + εu) 
   ≥ f(x) - ε‖∇f(x)‖ + εδ²/2‖∇f(x)‖ - ε‖∇f(x)‖δ²/4
   = f(x) - ε‖∇f(x)‖ + εδ²/4‖∇f(x)‖
   = f(x) + ε‖∇f(x)‖(δ²/4 - 1)      (1)

   Also by Taylor: 
   f(x + εp) 
   = f(x) + εp~∇f(x) + ϕ(εp), with lim ε→0 ϕ(εp)/ε = 0
   = f(x) - εp~p‖∇f(x)‖ + ϕ(εp)
   = f(x) - ε‖∇f(x)‖ + ϕ(εp)

   By Lemma, |ϕ(εp)| ≤ ε‖∇f(x)‖δ²/8
   ϕ(εp) ≤ ε‖∇f(x)‖δ²/8
   f(x + εp) 
   ≤ f(x) - ε‖∇f(x)‖ + ε‖∇f(x)‖δ²/8
   = f(x) + ε‖∇f(x)‖(δ²/8 - 1)      (2)

   By (1) and (2), we get f(x + εp) ≤ f(x) + ε‖∇f(x)‖(δ²/8 - 1) < f(x) + ε‖∇f(x)‖(δ²/4 - 1) ≤ f(x + εu)

 defn Descent Direction
  In general, pᵏ is a descent direction if
   f(xᵏ + εpᵏ) < f(xᵏ) ∀ε sufficiently small

 Thm
  Let xᵏ be such that ∇f(xᵏ) ≠ 0 If pᵏ~∇f(xᵏ) < 0, then pᵏ is a descent direction

  pf
   let p = pᵏ, assume wlog ‖p‖ = 1
    f(xᵏ + εp) = f(xᵏ) + εp~∇f(xᵏ) + ϕ(εp), with usual requirements for residual ϕ(εp)
    For sufficiently small ε |ϕ(εp)| ≤ ε|1/2p~∇f(xᵏ)| ≤ -εp~∇f(xᵏ)/2
    f(xᵏ + εp) ≤ f(xᵏ) + εp~∇f(xᵏ)/2 < f(xᵏ)

Line Search
 Once pᵏ is chosen, determine αᵏ in xᵏ⁺¹ = xᵏ + αᵏpᵏ

 Exact line search: αᵏ = argmin α ≥ 0 {f(xᵏ + αpᵏ)}
 Define Ψ(α) = f(xᵏ + αpᵏ)

 Notes
  Ψ(0) = f(xᵏ)
  once αᵏ is chosen, Ψ(αᵏ) = f(xᵏ⁺¹)
  Ψ'(α) = d/dα Ψ(α) = ∇f(xᵏ + αpᵏ)~pᵏ   (directional derivative)
  Ψ'(0) = ∇f(xᵏ)~pᵏ < 0, by assumption of pᵏ is a descent direction

=LEC16= 2018-10-19
 Sufficient decrease condition
  fix 0 < σ < 1/2
  α must satisfy
   Ψ(α) ≤ Ψ(0) + σαΨ'(0)

 Curvature condition:
  Ψ(2α) > Ψ(0) + σ2αΨ'(0)

 Armijo ("backtrack") inexact line search
  let α := 1
  if α fails sufficient decrease:
   while α fails sufficient decrease:
    α /= 2
  elsif α fails curvature condition:
   while α fails curvature condition:
    α *= 2

 Thm:
  Let f ∈ C¹, ∇f(xᵏ) ≠ 0. and let pᵏ be a descent direction.
  Either the armijo algorithm terminates, and α terminates both conditions, 
  or α → ∞ and f is unbounded below, that is f(xᵏ) → -∞

  pf
   If the first loop terminates, α satisfies sufficient decrease and 2α fails it.
   Therefore, α satisfies the curvature condition
   To show the first loop terminates:
    Ψ(α) = Ψ(0) + αΨ'(0) + ϕ(α)
   For α sufficiently small, 
    |ϕ(α)| ≤ α|1/2Ψ'(0)|
    ϕ(α) ≤ -αΨ'(0)/2, therefore Ψ(α) ≤ Ψ(0) + αΨ'(0)/2, exactly the sufficient decrease condition

   If the second loop terminates, α satisfies curvature condition, and α/2 does not, so α satisfies sufficient decrease
   If second loop does not terminate, Ψ(2ʲ) ≤ Ψ(0) + 2ʲσΨ'(0), ∀j ∈ Z, j ≥ 0
   Thus, Ψ(2ʲ) → -∞ for j → +∞

   If neither loop occurs, then α = 1 satisfies both conditions.

 defn Lipschitz Continuous
  A function f: Rⁿ → R is Lipschitz continuous with constant L if
   |f(y) - f(x)| ≤ ‖y - x‖L ∀x, y ∈ Rⁿ
  Note that this does not necessarily ensure f is C⁰ continuous.

 Thm
  Let f ∈ C¹(B_δ(0))
  f is Lipschitz continuous with constant L on B_δ(0) iff ‖∇f(x)‖ ≤ L, ∀x ∈ B_δ(0)

 Zoutendijk's Thm:
  Let f: Rⁿ → R with f ∈ C¹(Rⁿ), if
   1. ∇f is Lipschitz continuous
   2. ∀k, pᵏ is a descent direction with ∇f(xᵏ)~pᵏ ≤ -μ‖∇f(xᵏ)‖₂ ‖pᵏ‖₂ for some 0 < μ ≤ 1
   3. ∀k, αᵏ satisfies both sufficient decrease and curvature conditions
  Then either (a) lim f(xᵏ) = -∞ or (b) lim ∇f(xᵏ) = 0 as k → ∞

=LEC17= 2018-10-22

  pf:
   We want to prove that if (b) does not hold, then (a) does.
   Note that (b) is equivalent to
    ∀ε > 0, ∃K > 0, ∀k ≥ K, ‖∇f(xᵏ)‖ < ε, by the definition of a limit
   If instead lim k → ∞ ∇f(xᵏ) ∄ or ≠ 0, then 
    ∃ε > 0, ∀K > 0, ∃k ≥ K ‖∇f(xᵏ)‖ ≥ ε  (1)

   We will show that (1) implies f(xᵏ⁺¹) ≤ f(xᵏ) - δ for some constant δ > 0
   thus, f(xᵏ) → -∞

   A)
    Curvature condition: 
     Ψ(2αᵏ) > Ψ(0) + 2αᵏσΨ'(0), for 0 < σ ≤ 1/2
    Mean value theorem: 
     ∃ 0 ≤ γ ≤ 2αᵏ: Ψ(2αᵏ) = Ψ(0) + 2αᵏΨ'(γ)
    Together
    Ψ(0) + 2αᵏΨ'(γ) > Ψ(0) + 2αᵏσΨ'(0)
    Ψ'(γ) > σΨ'(0)                      (2)

   B)
    Lipschitz: 
     ‖∇f(xᵏ + γpᵏ) - ∇f(xᵏ)‖ ≤ Lγ‖pᵏ‖
    Cauchy-Schwartz inequality: 
     [∇f(xᵏ + γpᵏ) - ∇f(xᵏ)]~pᵏ ≤ ‖∇f(xᵏ + γpᵏ) - ∇f(xᵏ)‖ ‖pᵏ‖ ≤ Lγ‖pᵏ‖²
    Therefore,
    ∇f(xᵏ + γpᵏ)~pᵏ ≤ ∇f(xᵏ)~pᵏ + Lγ‖pᵏ‖²  (3)

   C)
    By (2) and (3)
    ∇f(xᵏ)~pᵏ + Lγ‖pᵏ‖² > σ∇f(xᵏ)~pᵏ
    Lγ‖pᵏ‖² > (σ-1)∇f(xᵏ)~pᵏ = (σ-1)Ψ'(0) = (1 - σ)(-Ψ'(0))
    γ = (1 - σ)(-Ψ'(0)) / L‖pᵏ‖²
    recalling that 0 ≤ γ ≤ 2αᵏ, therefore αᵏ ≥ γ/2

    αᵏ ≥ (1 - σ)(-Ψ'(0)) / 2L‖pᵏ‖²

   D)
    Sufficient decrease condition:
     Ψ(αᵏ)
     ≤ Ψ(0) + σαᵏΨ'(0)
     ≤ Ψ(0) + σ[(1 - σ)(-Ψ'(0)) / 2L‖pᵏ‖²]Ψ'(0)
     = Ψ(0) - [σ(1-σ)/2L][Ψ'(0)²/‖pᵏ‖²]

   By hypothesis, 
    [Ψ'(0)]² = [∇f(xᵏ)~pᵏ]² ≥ [μ‖∇f(xᵏ)‖.‖pᵏ‖]²
   
   Ψ(αᵏ) 
   ≤ Ψ(0) - [σ(1-σ)/2L][(μ‖∇f(xᵏ)‖.‖pᵏ‖)²/‖pᵏ‖²]
   = Ψ(0) - [σ(1-σ)/2L]μ²‖∇f(xᵏ)‖²

   By (1), ‖∇f(xᵏ)‖ ≥ ε
   so 
    Ψ(αᵏ) ≤ Ψ(0) - (σ(1-σ)μ²ε²)/2L
    f(xᵏ + αᵏpᵏ) ≤ f(xᵏ) - (σ(1-σ)μ²ε²)/2L
  
   Now we have a complete algorithm
   Start at arbitrary x⁰ ∈ Rⁿ
   for k = 1, 2, ...
    choose pᵏ s.t. ∇f(xᵏ)~pᵏ ≤ -μ‖∇f(xᵏ)‖.‖pᵏ‖ for some 0 < μ ≤ 1
     e.g. pᵏ = -∇f(xᵏ) [steepest descent]
    choose αᵏ with Armijo inexact line search
    let xᵏ⁺¹ = xᵏ + αᵏpᵏ
    if (f(xᵏ⁺¹) < -M) or (‖∇f(xᵏ⁺¹‖ < ε):
     STOP

=LEC18= 2018-10-24
Convergence of Descent Algorithms
 
 defn Convergence
  A sequence s⁰, s¹, ... converges with degree d to zero if |sᵏ⁺¹| ≤ C|sᵏ|ᵈ for some constant C
  Convergence is linear if d = 1, quadratic if d = 2
  For linear convergence, 0 < C < 1
  For quadratic convergence, 0 < C

 defn Strongly Convex
  (substituting l from lecture for c for readability purposes)

  A function f ∈ C¹(Rⁿ) is strongly convex if it satisfies
   (∇f(y) - ∇f(x))~(y-x) ≥ c‖y - x‖² ∀x, y ∈ Rⁿ for some c > 0

  Lemma: If f is strongly convex, then
   ‖∇f(y) - ∇f(x)‖₂² ≥ c|f(y) - f(x)|, ∀x,y ∈ Rⁿ
  Lemma: Let f ∈ C²(Rⁿ), f is strongly convex iff
   ∇²f(x) - cI is psd for all x ∈ Rⁿ
  Lemma: Any strongly convex function is strictly convex


 Thm:
  Assuming the same conditions as Zoutendijk's theorem:
  Let f: Rⁿ → R with f ∈ C¹(Rⁿ), if
   1. ∇f is Lipschitz continuous
   2. ∀k, pᵏ is a descent direction with ∇f(xᵏ)~pᵏ ≤ -μ‖∇f(xᵏ)‖₂ ‖pᵏ‖₂ for some 0 < μ ≤ 1
   3. ∀k, αᵏ satisfies both sufficient decrease and curvature conditions
   AND
   4. f is strongly convex
  Then f(xᵏ) converges linearly to a local minimum f(x*)

  pf: 
   from Zoutendijk proof, we have
    f(xᵏ⁺¹) ≤ f(xᵏ) - σ(1-σ)/2L ‖∇f(xᵏ)‖² 
    f(xᵏ⁺¹) - f(x*) ≤ f(xᵏ) - f(x*) - σ(1-σ)/2L ‖∇f(xᵏ)‖² 
   By Lemma
    ‖∇f(xᵏ⁺¹) - ∇f(x*)‖₂² ≥ c|f(xᵏ) - f(x*)|
    ‖∇f(xᵏ⁺¹)‖² ≥ c(f(xᵏ) - f(x*)), as ∇f(x*) = 0

   so
    f(xᵏ⁺¹) - f(x*) 
    ≤ f(xᵏ) - f(x*) - σ(1-σ)c(f(xᵏ) - f(x*))/2L 
    = f(xᵏ) - f(x*) (1 - σ(1-σ)c/2L)
   Note that 0 < σ(1-σ)c/2L < 1
   so f(xᵏ⁺¹) - f(x*) ≤ f(xᵏ) - f(x*)
   Thus f(xᵏ) - f(x*) converges linearly to zero.

 Thm:
  For a strongly convex, quadratic function, the steepest descent method with exact linear search has ‖xᵏ - x*‖ converge linearly to zero, on a tight bound (d > 1 does not converge)

 Newton step
  Consider a quadratic approximation of f at xᵏ
  f(xᵏ + h) ≈ q(h) = f(xᵏ) + h~∇f(xᵏ) + h~∇²f(x)h/2
  iff ∇²f(xᵏ) is pd, q(h) has a unique minimizer h = -[∇²f(xᵏ)]`∇f(xᵏ)
  The Newton step is given by taking pᵏ = -[∇²f(xᵏ)]`∇f(xᵏ), only when ∇²f(xᵏ) is pd

=LEC19= 2018-10-26
 #notation: A` = inverse of A

 According to Newton's method, xᵏ⁺¹ = xᵏ - ∇f(xᵏ)/∇²f(xᵏ)

 Lemma: Let F: Rⁿ → Rᵐˣᵐ be continuous over B_r(x⁰) for some x⁰ ∈ Rⁿ, s.t. F(x⁰) is nonsingular
  Then there exists x > 0 s.t. F(x) is invertible for all x ∈ B_r(x⁰) and F(x)` is continuous over B_r(x⁰)

  pf: 
   Given A ∈ Rᵐˣᵐ, det(A) is a polynomial in A₁₁, ... Aₘₘ, the entries of A.
   Since det(F(x₀)) ≠ 0, ∃r¹ > 0 s.t. det(F(x)) ≠ 0, ∀x ∈ B_r¹(x⁰)
   Note that A`ᵢⱼ = p(A₁₁, ..., Aₘₘ)/det(A), for some polynomial p
   Thus, F(x)` is a polynomial in F(x) divided by another nonzero polymial, so it is continuous

 Thm:
  Let f: Rⁿ → R be such that f∈ C²(B_r(x*))
  If
   a. ∇²f is Lipschitz continuous over B_r(x*), ‖∇²f(y) - ∇²f(x)‖₂ ≤ L‖y-x‖₂
   b. ∇f(x*) = 0 and ∇²f(x*) is pd [2nd order necessary conditions for local opt]
   c. ‖∇²f(x)`‖ ≤ 2‖∇²f(x*)`‖ ∀x ∈ B_r(x*) [by Lemma above, ∃r sufficiently small s.t. d is satisfied]
   d. r ≤ 1/2L‖∇²f(x*)`‖
  Then Newton's method converges quadratically to x* if x⁰ ∈ B_r(x*)

  pf
   Assume that xᵏ ∈ B_r(x*) (by induction)
   xᵏ⁺¹ - x* 
   = xᵏ - ∇f(xᵏ)/∇²f(xᵏ) - x*
   = ∇²f(xᵏ)`[∇²f(xᵏ)(xᵏ - x*) - ∇f(xᵏ) - ∇f(x*)]    # by part b of hypothesis, ∇f(x*) = 0
   = ∇²f(xᵏ)`[₀∫¹ ∇²f(xᵏ)(xᵏ - x*)dt - ₀∫¹ ∇²f(x* + t(xᵏ - x*))(xᵏ - x*)dt]
   = ∇²f(xᵏ)`₀∫¹ ∇²f(xᵏ)(xᵏ - x*) - ₀∫¹ ∇²f(x* + t(xᵏ - x*))(xᵏ - x*)dt

   ‖∇²f(xᵏ)`‖ ≤ 2‖∇²f(x*)`‖ by c
   ‖₀∫¹ [∇²f(xᵏ) - ∇²f(x* + t(xᵏ - x*))](xᵏ - x*)dt‖ 
   ≤ ₀∫¹‖[∇²f(xᵏ) - ∇²f(x* + t(xᵏ - x*))](xᵏ - x*)‖ dt   # extending triangle inequality to integrals
   ≤ ₀∫¹‖∇²f(xᵏ) - ∇²f(x* + t(xᵏ - x*))‖. ‖(xᵏ - x*)‖ dt
   ≤ ₀∫¹L‖(xᵏ - x* - t(xᵏ - x*))‖. ‖(xᵏ - x*)‖ dt
   = L ₀∫¹‖((1 - t)(xᵏ - x*))‖. ‖(xᵏ - x*)‖ dt
   = L ‖xᵏ-x*‖² ₀∫¹((1 - t)dt
   = 1/2L‖xᵏ - x*‖²
   
   ‖xᵏ⁺¹ - x*‖ 
   ≤ ‖∇²f(xᵏ)‖ ‖₀∫¹ *** ‖
   ≤ 2‖∇²f(x*)‖ 1/2L ‖xᵏ - x*‖²

   By induction, ‖xᵏ - x*‖ ≤ r, and by d, r ≤ 1/2L‖∇²f(x*)`‖
   So, ‖∇²f(x*)`‖ ≤ 1/2Lr
   So, ‖xᵏ⁺¹ - x*‖ ≤ 1/2r ‖xᵏ - x*‖², thus convergence, if it exists, is quadratic.
   Since ‖xᵏ - x*‖ ≤ r, we have ‖xᵏ⁺¹ - x*‖ ≤ 1/2‖xᵏ - x*‖, so convergence exists.

=LEC20= 2018-10-29
_CH7-Trust Region Methods_
Algorithm:
 Choose starting point x⁰ arbitrarily
 let δ⁰ = 1
 for k = 0, 1, ...
  Le q(x) be a quadratic approximation of f that is accurate around xᵏ
  TEST(x) = arg min x{q(x): ‖x - xᵏ‖ ≤ δᵏ}
  ρ = f(xᵏ) - f(TEST(x))/q(xᵏ) - q(TEST(x))

  if ρ ≥ 1/δᵏ:
   xᵏ⁺¹ = TEST(x)
  else:
   xᵏ⁺¹ = xᵏ

  if ρ ≤ 1/4:
   δᵏ⁺¹ = δᵏ/2
  else if ρ ≥ 3/4 and ‖TEST(x)-xᵏ‖ = δᵏ
   δᵏ⁺¹ = 2δᵏ
  else:
   δᵏ⁺¹ = δᵏ

 There are other possible choices, but we consider 
  q(x) = f(xᵏ) + (x - xᵏ)~∇f(xᵏ) + 1/2(x - xᵏ)~∇²f(xᵏ)(x - xᵏ)
 ρ is the ratio Δf/Δq from xᵏ to TEST(x)
 The decrease in q is ≥ 0, since q(xᵏ) is considered in the argmin.
 If TEST(x) = xᵏ, then 2nd order sufficient conditions are satisfied, so STOP
 δᵏ is the trust region radius
 We consider that q is a good approximation of f in B_δᵏ(xᵏ), If ρ is small, then approximation is bad, so we decrease δᵏ

 Thm
  Let f∈ C²(Rⁿ) and assume that ∇²f is Lipschitz continuous in a ball that contains the level set of x⁰.
  Then, for the trust region method,
   1. either f(xᵏ) → -∞ or ∇f(xᵏ) → 0
   2. if xᵏ → x*, then x* satisfies 1st and 2nd order necessary conditions for local optimality
   3. if xᵏ → x* and x* satisfies the 2nd order sufficient conditions for local optimality,
      then for sufficiently large k, ‖TEST(x) - xᵏ‖ < δᵏ, each step is a newton step, so convergence is quadratic

 The trust region subproblem (TRS)
  arg min x{f(xᵏ) + (x - xᵏ)~∇f(xᵏ) + 1/2(x - xᵏ)~∇²f(xᵏ)(x - xᵏ): ‖x - xᵏ‖ ≤ δᵏ}
  f(xᵏ) is constant, so can remove, doesn't affect arg min at all

  For simplicity, let x̅ = (x - xᵏ)/δᵏ
  We get 
    argmin {x̅ ~. (δᵏ1/2∇²f(xᵏ)) . x̅ + (δᵏ∇f(xᵏ))~x̅ : ‖x̅ ‖ ≤ 1}
  = argmin {x̅ ~ Ax̅ + b~x̅ : ‖x̅ ‖ ≤ 1}, where A = δᵏ1/2∇²f(xᵏ), b = δᵏ∇f(xᵏ)

  To solve TRS?
   min x~Ax + b~x: ‖x‖ = 1
  If A is pd, then can compute x^ = -1/2A`b

  Case 1: A is pd and ‖x^‖ ≤ 1
   Then x^ is optimal, very easy case.

  Case 2: A is not pd OR ‖x^‖ > 1

=LEC21= 2018-10-31
   Let x^(λ) = -1/2(A + λI)`b

   x^(0) would be optimal in case 1
   x^(λ) is a global minimizer for x~(A + λI)x + b~x
   Let λ₁ ≤ ... ≤ λₙ be the eigenvalues of A, x^ is defined for all λ > -λ₁

   Thm: ‖x^(λ₁)‖ is a decreasing function of λ over the interval (-λ, ∞), 
    and further, lim λ → ∞ ‖x^(λ)‖ = 0
    pf
     Let A = QDQ~ where Q is orthogonal and D = diag(λ₁, ..., λₙ)
     Observe that ∀z ∈ Rⁿ , ‖Qz‖ = ‖z‖ as ‖Qz‖² = (Qz)~(Qz) = z~Q~Qz = z~z = ‖z‖²

     Therefore, 
      x^(λ) 
      = -1/2(QDQ~ + λI)`b
      = -1/2(QDQ~ + λQIQ~)`b
      = -1/2(Q(D+λI)Q~)`b
      = -1/2 Q~`(D+λI)`Q`b
      = -1/2 Q(D+λI)`Q~b
 
      ‖x^(λ)‖
      = 1/2‖Q(D+λI)`Q~b‖
      = 1/2‖(D+λI)`Q~b‖      # by observation above
      = 1/2‖(D+λI)`c‖        # c = Q~b
      = 1/2√(∑cᵢ²/(λᵢ + λ)²)
      = 1/2 ‖[c₁/(λ₁ + λ), ..., cₙ/(λₙ + λ)]~‖
 
      notice that cᵢ is a nonnegative constant and 1/(λᵢ + λ) is decreasing for increasing λ, λ ≥ -λ₁
      So, ‖x^(λ)‖ is a decreasing function
      So, lim λ → ∞ ‖x^(λ)‖ = 0


   Case 2A: c₁ ≠ 0, and λ₁ ≠ 0
    c₁ = (Q~b)₁ = v₁~b, v₁ is the eigenvector of ∇²f(xᵏ) for λ₁
    if c₁ ≠ 0, then ∇f(xᵏ) is not orthogonal to v₁

    In this case, lim ‖x^(λ)‖ = ∞ as λ → -λ₁
    Then, ∃λ* : ‖x^(λ*)‖ = 1

   Lemma: If x^ is a global minimizer for TRS in case 2A, then ‖x^‖ = 1
    pf: if ‖x^‖ < 1, then ∃B_δ(x^) ⊆ B₁(0)
     since x^ is a global minimizer, it is also a local minimizer for x~Ax + b~x
     If λ₁ < 0, A is not psd and x~Ax + b~x has no local minimizers
     If λ₁ = 0, not allowed by hypothesis for case 2A
     If λ₁ > 1, A is pd and x~Ax + b~x has a unique local + global minimizer, and by case 2 hypothesis ‖x^‖ > 1
   
   Thm: 
    x^(λ*) is a global minimizer for TRS in case 2A.
    pf
     recall that x^(λ*) is a global minimizer for x~(A + λ*I)x + b~x
     If we restrict ‖x‖ = 1,
     x^(λ*) 
     = argmin {x~(A + λ*I)x + b~x: ‖x‖ = 1}
     = argmin {x~Ax + λ*x~x + b~x, ‖x‖ = 1}
     = argmin {x~Ax + b~x}                  # x~x = 1, so constant λ* doesnt change the argmin

   Case 2B: c₁ = 0 OR λ₁ = 0
    Thm:
     A global minimzer for TRS in case 2B is given by
      x^ = ∑ v₁~b/(λᵢ - λ₁) + τv₁, for all i such that λᵢ ≠ λ₁,
      ‖vᵢ‖ = 1, and τ is chosen such that ‖x^‖ = 1

     pf: Nocedal-Wright, "the hard case"

MIDTERM 2: Monday November 12th
ASSIGNMENT 2: Due November 12th

=LEC22= 2018-11-02
_CH8-Optimality Conditions for Constrained Optimization_

defn Constrained Local Minimizer
 Consider min {f(x): x ∈ G}, where G ⊆ Rⁿ.
 The point x* is a local minimzer if x* ∈ G and there exists ε > 0 s.t. ∀x ∈ B_ε(x*) ∩ G, f(x) ≥ f(x*)
 
 Note: 
  This does not require that B_ε(x*) ∩ G ≠ {x*}
  Equivalently, ∄d ∈ B_ε(0): x* + d ∈ G and f(x* + d) < f(x*)
  such a d would be a feasible improving direction (or step)

 Informally, consider min{f(x): h(x) = 0}
  Let y ∈ Rⁿ s.t. h(y) = 0                        #using y instead of x̅ as on whiteboard for ease of notation
  Is there an improving direction d at y?

  If d is small h(y + d) ≈ h(y) + d~∇h(y)
 
  d feasible if d~∇h(y) = 0
  d improving if d~∇f(y) < 0

  Take some arbitrary d orthogonal to ∇h(y)
  if d~∇f(y) < 0, done.
  if d~∇f(y) > 0, take -d, which will satisfy improving condition, still feasible
  if d~∇f(y) = 0, choose another d

  There are no feasible improving directions:
   when ∀d ∈ Rⁿ: d~∇h(y) = 0, we have d~∇f(y) = 0
   when all directions orthogonal to ∇h(y) are also orthogonal to ∇f(y)
   when ∇h~(y) is parallel to ∇f(y)

   Such a y is called a Karush-Kuhn-Tucker (KKT) point

  Example:
   min{x₁ + x₂: x₁² + x₂² - 2 = 0}

   ∇f(x) = [1 1]~
   ∇h(x) = [2x₁ 2x₂]~
   KKT points: x₁ = x₂, so x ∈ {[1, 1], [-1, -1]}

 Informally, consider min{f(x): g(x) ≤ 0}
  Let y be such that g(y) ≤ 0

  Case 1: g(y) < 0
   Then for a sufficiently small ‖d‖, g(y + d) < 0
   We want d~∇f(y) < 0, which exists iff ∇f(y) ≠ 0
  Case 2: g(y) = 0
   d feasible if g(y + d) + d~∇g(y), so we want d~∇g(y) ≤ 0
   d improving if d~∇f(y) < 0

  No feasible improving directions:
  Case 1: ∇f(y) = 0
  Case 2: ∀d: d~∇g(y) ≤ 0, d~∇f(y) ≥ 0

  Lemma: Let a, b ∈ Rⁿ, the following are equivalent
   1. ∀d∈ Rⁿ d~a ≤0 ⇒ d~b ≥ 0
   2. ∃λ: b = -λa, λ ≥ 0

  Case 1: ∇f(y) = 0
  Case 2: ∇f(y) = -λ∇g(y) for some λ ≥ 0

 KKT Points: ∇f(y) = -λ∇g(y) OR λg(y) = 0, λ ≥ 0

=LEC23= 2018-11-05

 Recap:
  Given min{f(x): g(x) ≤ 0} KKT at y are:
   Case g(y) < 0: ∇f(y) = 0
   Case g(y) = 0: ∇f(y) = -λ∇g(y) for some positive λ
 
 Example:
  min {x₁ + x₂: x₁² + x₂² - 2 ≤ 0}

  Case 1: x₁² + x₂² - 2 < 0
   ∇f(y) = [1, 1]~ = 0 never holds

  Case 2: x₁² + x₂² - 2 = 0
   ∇f(y) = [1, 1]~ = -λg(y) = -λ[2y₁, 2y₂]~
   [-λ/2, -λ/2]~ = [y₁, y₂]~
   y = [-1, -1]~ is the only KKT point.

  defn Non Linear Problem (NLP)
   min f(x): 
    gᵢ(x) ≤ 0 ∀i ∈ [1, 2, ..., m]
    hⱼ(x) = 0 ∀j ∈ [1, 2, ..., p]

  defn Linearized Feasible Direction
   Let y be feasible for (NLP)
   A linearized feasible direction is a vector d ∈ Rⁿ:
    i)  ∀i ∈ [1, 2, ..., m]: if gᵢ(y) = 0, d~∇gᵢ(y) ≤ 0
    ii) ∀j ∈ [1, 2, ..., p]: d~∇hⱼ(y) = 0

   The cone of linearized feasible directions at y is the set of all such directions, denoted L_(NLP)(y)

  defn KKT Point
   Let y ∈ Rⁿ,
   y is a KKT point if it satisfies the KKT conditions:
    i)  y is feasible for (NLP)
    ii) ∀d ∈ L_(NLP)(y), d~∇f(y) ≥ 0

  Farkas' Lemma
   Given A ∈ Rᵐˣⁿ, b ∈ Rᵐ
   {Ax = b, x ≥ 0} is feasible iff {A~y ≥ 0, b~y < 0} is infeasible

   pf:
    (=>)
        #notation: x in this section is x̅ in whiteboard notes 
    Let x be such that Ax = b, x ≥ 0
    Then ∀ y ∈ Rᵐ, if A~y ≥ 0: x~A~y ≥ x~0 ≥ 0
    x~A~y ≥ 0
    (Ax)~y ≥ 0
    b~y ≥ 0
    Therefore, {A~y ≥ 0, b~y < 0} is infeasible

    (<=)
    Consider the primal-dual pair
    (P) = min {0~x: Ax = b, x ≥ 0}
    (D) = max {b~y: A~y ≤ 0}
    Note that (P) cannot be unbounded, as 0~x is always 0.
    Note that (D) cannot be infeasible, as y = 0 is feasible.

    {Ax = b, x ≥ 0} being infeasible 
    ⇒ (P) is infeasible, 
    ⇒ (D) is unbounded
    ⇒ ∃d ∈ Rᵐ: A~d ≤ 0 and b~d > 0
    Let y = -d, so A~y ≥ 0, and b~t < 0
    ⇒ {A~y ≥ 0, b~y < 0} is feasible

    The contrapositive proves this.

 Thm:
  Let A ∈ Rᵐˣⁿ, B ∈ Rᵐˣᵖ, b∈ Rᵐ
  {Ax + Bw = b, x ≥ 0} is feasible iff {A~y ≥ 0, B~y = 0, b~y < 0} is infeasible

  pf:
   {Ax + Bw = b, x ≥ 0} is feasible
   ⟺ {Ax + Bw⁺ - Bw⁻ = b, x, w⁺, w⁻ ≥ 0} is feasible
   ⟺ {[A B -B][x w⁺ w⁻]~ = b, [x w⁺ w⁻]~ ≥ 0} is feasible
   ⟺ {[A B -B]~y ≥ 0, b~y ≥ 0} is infeasible
   ⟺ {A~y ≥ 0, B~y ≥ 0, -B~y ≥ 0, b~y ≥ 0} is infeasible
   ⟺ {A~y ≥ 0, B~y = 0, b~y ≥ 0} is infeasible

=LEC24= 2018-11-07
  
 KKT conditions at y feasible for (NLP):
  Consider the system (1)
  For all d s.t. 
   d~∇gᵢ(y) ≤ 0, i = 1, ..., m: gᵢ(y) = 0
   d~∇hⱼ(y) = 0, j = 1, ..., p
  This implies d~∇f(y) ≥ 0

  Consider the system (2)
   -∇gᵢ(y)~d ≥ 0, i ∈ [1, 2, ..., p] gᵢ(y) = 0 
   ∇hⱼ(y)~d  = 0, j ∈ [1, 2, ..., p]
   ∇f(y)~d < 0

  By Farka's Lemma, this is infeasible iff ∃λᵢ ∈ Rᵐ, λᵢ ≥ 0, μ ∈ Rᵐ:
   -∑λᵢ∇gᵢ(y)     + ∑μ∇hⱼ(y) = ∇f(y)
    i: gᵢ(x) = 0        

 Thm:
  Given (NLP), a feasible point y is a KKT point iff ∃λ ∈ Rᵐ, λ ≥ 0, μ ∈ Rᵖ:
   -∑λᵢ∇gᵢ(y) + ∑μ∇hᵢ(y) = ∇f(y)                # differs from previous expression, first sum is now over all i's
   λᵢgᵢ(y) = 0

  Example
   min {c~x: Ax = b, x ≥ 0}
   
   equivalently
   min {c~x: -Ix ≤ 0, Ax - b = 0}
   gᵢ(x) = -xᵢ, ∇gᵢ(x) = -eᵢ
   hⱼ(x) = Aᵢ~x - bᵢ, ∇hⱼ(x) = Aᵢ~

   KKT conditions ∃λ ≥ 0, μ:
    ∑λᵢeᵢ + ∑μᵢAᵢ~ = c, λ(-yᵢ) = 0
    ⟺ {λI + A~μ = c, y~A = 0, λ ≥ 0}
    ⟺ {A~μ ≤ c, (c - A~μ)~y = 0}          # choosing λ = c - A~μ


 Let Ω = {x ∈ Rⁿ: gᵢ(x) ≤ 0 ∀i, hᵢ(x) = 0 ∀i} # feasible region of (NLP)

 defn Feasible Arc
  A feasible arc at x in the direction of d is a function ϕ: [0, c] → Rⁿ for some c > 0, where
   1) ϕ(0) = x
   2) ϕ ∈ C¹([0, c])
   3) ϕ'(0) = d
   4) ϕ(t) ∈ Ω ∀t ∈ [0, c]

 defn Tangent Cone
  given a feasible point x ∈ Rⁿ, the tangent cone to Ω at x is 
   T_Ω(x) = {d ∈ Rⁿ: ∃ feasible arc at x with direction d}

 Example:
  Ω = {x ∈ R²: ‖x‖ ≤ 1} (unit circle)
  choose point [-1, 0]~
  T_Ω([-1, 0]~) = {[d₁, d₂]~: d₁ ≥ 0}

 Thm:
  Let x be feasible for (NLP) and assume L_(NLP) = T_Ω
  Then if x is a local minimizer of (NLP), then it is a KKT point

=LEC25= 2018-11-09
 Lemma: Let ϕ: R→Rⁿ and f: Rⁿ → R, with ϕᵢ ∈ C¹(R) for all i, and f ∈ C¹(Rⁿ)
  [d/dt f(ϕ(t))](t₀) = ∇f(ϕ(t₀))~(d/dt ϕ)(t₀)

  pf:
   By chain rule, given functions a, b: 
    a(b(x))'(x₀) = a'(b(x₀)) b'(x₀)
   Also by chain rule, given a(y₁, y₂):
    a(b(x), c(x))'(x₀) = δ/δy₁ a(b(x₀), c(x₀)) b'(x₀) + d/dy₂ a(b(x₀), c(x₀)) c'(x₀)

   So
    [d/dt f(ϕ(t))](t₀) 
    = [d/dx₁ f(ϕ(t₀))](d/dt ϕ)(t₀) + ... + [d/dxₙ  f(ϕ(t₀))](d/dt ϕ)(tₙ)
    = ∇f(t₀)~ϕ'(t₀)

 Thm: (from last lecture)
  Let x be feasible for (NLP) and assume L_(NLP) = T_Ω
  Then if x is a local minimizer of (NLP), then it is a KKT point

  pf:
   By definition of a KKT point, we want a feasible x and all d ∈ L_(NLP)(x): d~∇f(x) ≥ 0
   Let d ∈ L_(NLP)(x), then d ∈ T_Ω(x) so ∃ϕ feasible arc at x with direction d

   Let γ(t) = f(ϕ(t)), for t ≥ 0

   γ'(0) = lim (γ(t) - γ(0))/t
           t → 0, t > 0
   By definition of γ, γ(t) = f(ϕ(t)) and γ(0) = f(ϕ(0)) = f(x)
   Since x is a local minimzer, γ(t) - γ(0) = f(ϕ(t)) - f(x) ≥ 0, thus γ'(0) ≥ 0

   By Lemma (chain rule), γ'(0) = ∇f(ϕ(0))~ϕ'(0) = ∇f(x)~d
   Therefore, ∇f(x)~d ≥ 0

 Example where minimizer is not a KKT point:
  min x₁ + x₂
        - x₂ ≤ 0
   -x₁³ + x₂ ≤ 0
  Minimizer at [0, 0]~ = x*

  f(x) = x₁ + x₂, ∇f(x) = [1, 1]~, ∇f(x*) = [1, 1]~
  g₁(x) = -x₂, ∇g₁(x) = [0, -1]~, ∇g(x*) = [0, -1]~
  g₂(x) = -x₁³ + x₂, ∇g₂(x) = [-3x₁², 1]~, ∇g₂(x*) = [0, 1]~

  KKT gradient equation system: 
   ∇f(x*) = -λ₁g₁(x*) - λ₂g₂(x*)
   λ₁g₁(x*) = 0 
   λ₂g₂(x*) = 0

  ⟺ -λ₁[0, -1] - λ₂[0, 1] = [1, 1] which is infeasible, so x* cannot be a KKT point


 Note:
  In the example,
  T_Ω(x*) = {x ∈ R²: x₁ ≥ 0, x₂ = 0}
  L_(NLP)(x*) = {x ∈ R²: x₂ = 0}

  These two cones are distinct, 
   the tangent cone only cares about the feasible region, 
   the cone of linearized feasible directions cares about the gradients of the specific problem

 Example 2:
  min x₁ + x₂
        - x₂ ≤ 0
   -x₁³ + x₂ ≤ 0
   -x₁       ≤ 0
  
  g₃(x) = -x₁, ∇g₃(x) = [-1, 0]~, ∇g(x*) = [-1, 0]~
  gradient equation:
   ∇f(x*) = -λ₁[0, -1] - λ₂[0, 1] - λ₃[1, 0] = [1, 1], which is feasible
   Therefore, x* is a KKT point!

 Thm
  ∀x ∈ Ω, T_Ω(x) ⊆ L_(NLP)(x)

=LEC26= 2018-11-14
 Constrained Optimization

 Review:
  Given NLP:
   min f(x)
    s.t. 
    gᵢ(x) ≤ 0, i ∈ {1, .., m}
    hⱼ(x) = 0, j ∈ {1, .., p}

  T_Ω(x) = {d ∈ Rⁿ: ∃feasible arc Φ: Φ'(0) = d}
  L_NLP(x) = {d ∈ Rⁿ: ∇gᵢ(x)~d ≤ 0, ∀i s.t. gᵢ(x) = 0, ∇hⱼ(x)~d = 0 ∀j}
 
  KKT point:
   x ∈ Ω is a KKT point if ∇f(x)~d ≥ 0, ∀d ∈ L_NLP(x)
   ⟺ ∃λᵢ, μᵢ where all these hold:
    -∑λᵢ∇gᵢ(x) + ∑μᵢhᵢ(x) = ∇f(x)
    λᵢ ≥ 0
    λᵢgᵢ(x) = 0 ∀i
 
  Thm: Let x ∈ Ω st. T_Ω(x) = L_NLP(x) then
   if x is a local minimizer, then x is a KKT point.


 defn Constraint Qualification
  A constraint qualification (CQ) is a condition on the feasible set of NLP s.t. T_Ω(x) = L_NLP(x)

 Thm: Let x ∈ , then T_Ω(x) ⊆ L_NLP(x)
  pf:
   Let x ∈ Ω and let d ∈ T_Ω(x)
   There exists c > 0 and Φ: [0, c] s.t. 
    Φ(0) = x
    Φ is C⁰ smooth and Φ'(0) = d
    Φ(t) ∈ Ω ∀t ∈ [0, c]

   we want d ∈ L_NLP(x):
    ∇gᵢ(x)~d = 0, ∀i s.t. gᵢ(x) = 0
    ∇hⱼ(x)~d = 0, ∀j

   Suppose ∃i: gᵢ(x) = 0, consider Taylor epantion gᵢ∘Φ at 0 in the direction t ∈ [0, c]
   #notation: t here is t̅ in whiteboard notes

   define a function o(t) where lim t→0 o(t)/t = 0

   gᵢ(Φ(t)) = gᵢ(Φ(0)) + gᵢ'(Φ(0))t + o(t)
   Note that gᵢ(Φ(t)) ≤ 0 and that gᵢ(Φ(0)) = gᵢ(x) = 0
   
   0 ≥ gᵢ'(Φ(0))t + o(t)
     = ∇gᵢ(Φ(0))~Φ'(0)t + o(t)
     = ∇gᵢ(Φ(0))~dt + o(t)

   divide both sides by t
   0 ≥ ∇gᵢ(Φ(0))~d + o(t)/t

   limit t towards 0 both sides
   0 ≥ ∇gᵢ(Φ(0))~d + lim t → 0 o(t)/t
     = ∇gᵢ(x)~d, by definition of o

   Exercise: do for hⱼ(x)

 defn Linear Independence CQ (LICQ)
  The LICQ holds at x ∈ Ω if the set {∇gᵢ(x): gᵢ(x) = 0} ∪ {∇hⱼ(x): ∀j} is linearly independent

 Thm: Let x ∈ Ω
  if x satisfies LICQ, then T_Ω(x) = L_NLP(x)
 
  pf: read up on it :) 

 Note: h(x) = 0 ⟺ h(x) ≤ 0, -h(x) ≤ 0

 Example:
  min {x₁ + x₂: -x₂ ≤ 0, -x₁³ + x₂ ≤ 0}

  x* = [0, 0]

  Does the LICQ hold at x*?
  ∇g₁(x) = [0, -1]
  ∇g₂(x) = [-3x₁², 1]
  at x*: {[0, -1], [0, 1]}
  Clearly not linearly independent

 defn Linear Programing CQ (LPCQ)
  The LPCQ holds at x ∈ Ω if all the tight constraints are affine (of form ax - b)

 Thm:
  Let x ∈ Ω, if the LPCQ holds at x, then T_Ω(x) = L_NLP(x)

  pf:
   Let x ∈ Ω, where LPCQ holds at x.
   Then by defintion of LPCQ, we have
    gᵢ(x) < 0, i ∈ {1, ..., k} # for some k
    gᵢ(x) = 0, i ∈ {k+1, ..., m} ⟹ gᵢ(x) = aᵢ~x + bᵢ

    hⱼ(x) = 0 ∀j ⟹ hⱼ(x) = aⱼ~x -bⱼ

   Let d ∈ L_NLP(x), we want to prove d ∈ T_Ω(x)
   by definition of L_NLP, we have
    0 ≥ ∇gᵢ(x)~d = aᵢ~d: i ∈ {k + 1, ..., m}
    0 = ∇hⱼ(x)~d = aⱼ~d: ∀j

   Consider Φ(t) = x + td, we have Φ(0) = x, smooth with Φ'(t) = d, Φ(t) ∈ Ω
   ∀j: hⱼ(x + td) 
    = aⱼ~(x + td) - bⱼ = aⱼ~x - bⱼ + aⱼ~td = aⱼ~x - bⱼ + 0 = aⱼ~x - bⱼ = 0
   ∀i ∈ {k + + 1, .., m}: gᵢ(x + td)
    = aᵢ~(x + td) - bⱼ = aᵢ~x - bᵢ + aᵢ~td = gᵢ(x) + t∇gᵢ(x)~d ≥ 0
   ∀i ∈ {1, ..., k}: 
    gᵢ(x + td) ≤ 0, ∀t ∈ [0, εᵢ), εᵢ > 0, by continuity of gᵢ
   Then Φ is a feasible arc ∀t ≤ min{εᵢ}, so d ∈ T_Ω(x)

 Thm:
  Let x ∈ Ω s.t. a CQ holds at x, then if x is a local minimizer, x is also a KKT point.

=LEC27= 2018-11-16
 Constraint Qualifications cntd.

 Example
  min {x~Ax: ‖x‖² = 1} x ∈ Rⁿ, A ∈ Rⁿˣⁿ symmetric

  We have LICQ satisfied
  KKT satisfied at y if ∃μ: 
   2Ay = μ2y
   Ay = μy

   at y, objective value is y~Ay = y~(μy) = μy~y = μ

 
 Convex NLP (CP):
  min f(x)
   gᵢ(x) ≤ 0, i = 1..m
   hⱼ(x) = 0, j = 1..p
  f, gᵢ are convex
  hⱼ are affine

 defn Slater CQ
  The Slater CQ holds for (CP) if ∃y: gᵢ(y) < 0 ∀i = {1..m} (also known as strict feasibility)

 Thm:
  If the Slater CQ holds for (CP), then T_Ω(x) = L_NLP(x) ∀x ∈ Ω

 Thm: 
  If x is a KKT point for (CP) then x is a global minimizer of (CP)

  pf:
   Let y ∈ Ω, we want f(x) ≤ f(y)

   Since x is a KKT point, it follows that ∇f(x)~d ≥ 0 ∀d ∈L_NLP(x)
   Now we want to show that d = (y - x) ∈ L_NLP(x)

   Recall that if c ∈ C¹ and is a convex function, then
    c(x) ≥ c(y) + ∇c(y)~(x - y) ∀x, y

   Suppose some tight constraint gᵢ exists, that is gᵢ(x) = 0
   Then by the above observation, 
    gᵢ(y) ≥ gᵢ(x) + ∇gᵢ(x)~(y - x)
   By choice, gᵢ(x) = 0
   As y is a feasible point, gᵢ(y) ≤ 0
   so, we get
   0 ≥ ∇gᵢ(x)~(y - x)

   Consider j ∈ {1..p}, we want ∇hⱼ(x)~(y - x) = 0
   Since hⱼ is affine, hⱼ(x) = aⱼ~x + bⱼ
   h(x) = 0 ⇒ aⱼ~x + bⱼ = 0
   h(y) = 0 ⇒ aⱼ~y + bⱼ = 0
   ∇h(x) = aⱼ

   h(y) - h(x) = aⱼ~(y - x) = 0 = ∇h(x)~(y - x)

   We have shown that d = (y - x) ∈ L_NLP(X)
   by the kkt conditions:
    ∇f(x)~d ≥ 0
    ∇f(x)~(y - x) ≥ 0

   Since f is convex, again by previous observation, we get
   f(y) ≥ f(x) + ∇f(x)~(y - x)
        ≥ f(x)

   So, x must be a global minimizer. QED

 Corollary
  Suppose the Slater CQ holds for (CP), then x is a global minimizer iff x is a KKT point

 Remark: All results for convex optimization also hold when the functions are not C¹

 Example:
  min ‖x - x⁰‖₂²
   s.t. lᵢ ≤ xᵢ ≤ uᵢ, ∀i ∈ {1..n}, assume lᵢ < uᵢ 
  
  ∇f(x) = 2(x - x⁰), ∇²f(x) = 2I ≥ 0
  gᵢ+ = xᵢ - uᵢ ≤ 0 ⇒ eᵢx - uᵢ ≤ 0
  gᵢ- = -xᵢ + lᵢ ≤ 0 ⇒ -eᵢx + lᵢ ≤ 0
  All convex, so this is (CP)
  choose each xᵢ = 1/2(lᵢ + uᵢ) to satisfy Slater conditions
 
  2(x - x⁰) = -∑λᵢ+~eᵢ + -∑λᵢ-~(-eᵢ)
  x = 1/2(x⁰ - ∑(λᵢ- + λᵢ+)eᵢ)

 Exercise: finish KKT conditions for example above.